{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d2718f",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70721e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Validation \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61162e1",
   "metadata": {},
   "source": [
    "## ANN - Multilayer Perceptron\n",
    "\n",
    "### Pre-processing\n",
    "Wir verwenden für diesen Teil wieder die Grundwasserqualitätsdaten, die wir bereits in Termin 4 für die Binär-Klassifikation verwendet haben. Ziel ist es, mit Hilfe der anderen Parameter jeweils die Landnutzung (d.h. landwirtschaftlich ja/nein) in Form einer Binär-Klassifikation sowie die Nitrat-Gehalte in Form einer Regression vorherzusagen.\n",
    "\n",
    "Die Daten sind bereits teilweise pre-processed (kategorische Variablen sind encodiert, Werte unter der NWG wurden ersetzt). Wir prüfen nun noch, ob Nan-Werte vorhanden sind und löschen anschließend noch nicht benötigte Spalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6db7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einladen der Daten\n",
    "filename = 'gwdata_2005.csv'\n",
    "dataset = pd.read_csv(filename, delimiter=';')\n",
    "\n",
    "# Prüfen auf Nan-Werte\n",
    "dataset.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "\n",
    "dataset_cleaned = dataset.dropna()\n",
    "\n",
    "mydata = dataset_cleaned.drop(['GWNum','Messstelle','Rechtswert', 'Hochwert', 'Aquifer','Aquifer2','landuse'], axis=1)\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb724902",
   "metadata": {},
   "source": [
    "## Klassifikation\n",
    "### Splitten der Daten\n",
    "\n",
    "Das Aufteilen des Datensatzes erfolgt mit der Funktion \"train_test_split\" des Packages sklearn.\n",
    "\n",
    "Hier kann mit dem Parameter \"test_size\" das Verhältnis festgelegt werden und mit dem Parameter \"random_state\" die Reproduzierbarkeit garantiert werden. Der Testdatensatz dient später dazu, zu evaluieren, wie gut das trainierte neuronale Netz generalisieren kann, d.h. Daten vorhersagen, die es vorher \"noch nie gesehen hat\".\n",
    "\n",
    "Um Overfitting zu vermeiden und/oder das ANN hinsichtlich sog. Hyperparameter zu optimieren müssen wir die vorhandenen Daten weiter in einzelne Datensätze aufteilen. Je nach Zusammenhang bzw. Verwendung werden diese als Stopp-Set, Optimierungs-Set oder als Validation-Set bezeichnet. Diese können ebenfalls mit der Funktion train_test_split erzeugt werden.\n",
    "\n",
    "Für den Anfang wollen wir lediglich einen weiteren Datensatz, um Overfitting zu vermeiden. Dieser wird später bei jedem Trainingsschritt parallel beobachtet. Das ursprüngliche Training-Set (zunächst als temp bezeichnet) teilen wir dazu erneut auf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae572e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, test = train_test_split(mydata, test_size=0.2, random_state=27)\n",
    "\n",
    "train, val = train_test_split(temp, test_size=0.1, random_state=27)\n",
    "\n",
    "print('Größe Trainings-Set: ', train.shape)\n",
    "print('Größe Validation-Set: ', val.shape)\n",
    "print('Größe Test-Set: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fb03b",
   "metadata": {},
   "source": [
    "### Skalieren\n",
    "\n",
    "Für neuronale Netze müssen die Eingangsdaten skaliert werden. Je nach Datensatz und Fragestellung kann man die Daten entweder Normieren oder Standardisieren (siehe Übung Termin 2). Wir wählen hier die Normierung, damit unsere landuse_num Kategorien erhalten bleiben.\n",
    "\n",
    "Der Scaler wird zunächst an die Daten des Trainings-Set gefittet. Anschließend werden sowohl Trainings-Set,  Validation-Set und Test-Set transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f7291",
   "metadata": {},
   "source": [
    "### Aufteilen der Daten in Input- und Zieldaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cefbd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[:, :-1] # for all but last column --> Input variables\n",
    "y_train = train[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_val = val[:, :-1] # for all but last column --> Input variables\n",
    "y_val = val[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_test = test[:, :-1] # for all but last column --> Input variables\n",
    "y_test = test[:, -1] # for last column --> Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb254ac",
   "metadata": {},
   "source": [
    "### Aufbau und Training des ANN\n",
    "\n",
    "Wir importieren zunächst weitere nötige Pakete, und lesen dann die Anzahl der Input-features aus. Anschließend erstellen wir ein einfaches ANN mit einer Eingabeschicht (welche automatisch auf Basis der Anzahl der Input-Features erzeugt wird), einer verdeckten Schicht mit 5 Neuronen und einer Ausgabeschicht mit einem Neuron für unseren Zielwert (Nitrat). Vollständig miteinerander verbundene Schichten (fully connected layers) werden in Keras als Dense-Layer bezeichnet.\n",
    "\n",
    "Weitere Hyperparameter des Modells (neben der Anzahl der Schichten und Neuronen) sind die Initialisierungsfunktion, die Aktivierungsfunktion, der Optimizer und die Batch-Size (Anzahl der Inputs, die dem Netz pro Epoche auf einmal präsentiert werden). Diese werden zunächst relativ wahllos gewählt. Als Aktivierungsfunktion verwenden wir \"ReLu\" für die verdeckte Schicht, was für die meisten Problem in der Regel gut funktioniert. Für die Ausgabeschicht verwenden wir eine sigmoidale Aktivierungsfunktion, um sicherzustellen, dass die Ausgabe des Netzwerks zwischen 0 und 1 liegt und sich leicht auf eine der beiden Klassen mit einem Standard-Schwellenwert von 0,5 abbilden lässt. Der Adam-Optimizer gilt ebenfalls für viele Probleme als Methode der Wahl. Bei der Batch-Size findet man häufig Anfangswerte von 16, 32 oder 64. Alle Hyperparameter können später \"optimiert\" werden. Als loss-Funktion kommt für die binäre Klassifikation \"binary_crossentropy\" zum Einsatz, als Metrik die \"accuracy\". Zunächst starten wir mit 50 Epochen.\n",
    "\n",
    "Nach dem eigentlichen Training (model.fit) führen wir mit model.evaluate eine Vorhersage mit den validation-Inputs aus, um einen Hinweis dafür zu bekommen, wie gut das Modell ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75799c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# set initializer\n",
    "kernel_initializer = initializers.glorot_uniform()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', kernel_initializer=kernel_initializer, input_shape=(n_features,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1, validation_data=(X_val,y_val))\n",
    "\n",
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(X_val)\n",
    "\n",
    "# compute accuracy\n",
    "_, accuracy = model.evaluate(X_val,y_val)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "\n",
    "# plot loss learning curves\n",
    "\n",
    "pyplot.figure(figsize=(8,8)) \n",
    "pyplot.subplot(211)\n",
    "pyplot.title('Cross-Entropy Loss', pad=-40)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "# plot accuracy learning curves\n",
    "pyplot.subplot(212)\n",
    "pyplot.title('Accuracy')\n",
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9d886",
   "metadata": {},
   "source": [
    "Falls wir sehen, dass ein Overfitting eintritt, können wir die Anzahl der Epochen von Hand anpassen und einen zweiten Durchlauf starten (später mehr zu Techniken, wie man das Training im Fall von Overfitting automatisch stoppen kann).\n",
    "\n",
    "Die Accuracy beträgt bei diesem sehr einfachen und nicht optimierten Modell für die Validierungsdaten immerhin etwa 66% und ist damit in etwa vergleichbar mit der Performance des SVM/Scaled SVM aus Termin 4 (aber nicht so gut wie Random Forest). Da diese Konfiguration ganz gut zu funktionieren scheint, bauen wir das Modell nochmals neu auf, trainieren es mit dem kompletten Trainingsdatensatz (inklusive der Validierungs-Daten) und sagen damit die Landnutzung unseres Test-Datensatzes voraus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da13bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(mydata, test_size=0.2, random_state=27)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "# Input- and Target-Data\n",
    "X_train = train[:, :-1] # for all but last column --> Input variables\n",
    "y_train = train[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_test = test[:, :-1] # for all but last column --> Input variables\n",
    "y_test = test[:, -1] # for last column --> Target variable\n",
    "\n",
    "\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# set initializer\n",
    "kernel_initializer = initializers.glorot_uniform()\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', kernel_initializer=kernel_initializer, input_shape=(n_features,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1)\n",
    "\n",
    "# make class predictions with the model\n",
    "#predictions = model.predict_classes(X_val)\n",
    "\n",
    "# compute accuracy\n",
    "_, accuracy = model.evaluate(X_test,y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66be641",
   "metadata": {},
   "source": [
    "Auch dies scheint offensichtlich ähnlich gut zu funktionieren, die Vorhersage-Genauigkeit für den Test-Datensatz beträgt in diesem Durchlauf etwa 66 %. Die eigentlichen Daten können je nach Durchlauf aufgrund der unterschiedlichen Initialisierungen variieren. Wie man das Modell robuster macht, dazu bei der Regression mehr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d9537",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Splitten der Daten\n",
    "\n",
    "Zunächst sortieren für die Regression nun die Spalten so um, dass die Spalte mit den Werten, die wir später vorhersagen wollen, in diesem Fall Nitrat, am Ende steht. Das macht uns  die Aufteilung in Input- und Zieldaten leichter. Anschließend wiederholen wir die  Skalierung und Aufteilung der Daten.\n",
    "\n",
    "Das Aufteilen des Datensatzes erfolgt mit der Funktion \"train_test_split\" des Packages sklearn.\n",
    "\n",
    "Hier kann mit dem Parameter \"test_size\" das Verhältnis festgelegt werden und mit dem Parameter \"random_state\" die Reproduzierbarkeit garantiert werden. Der Testdatensatz dient später dazu, zu evaluieren, wie gut das trainierte neuronale Netz generalisieren kann, d.h. Daten vorhersagen, die es vorher \"noch nie gesehen hat\".\n",
    "\n",
    "Um Overfitting zu vermeiden und/oder das ANN hinsichtlich sog. Hyperparameter zu optimieren müssen wir die vorhandenen Daten weiter in einzelne Datensätze aufteilen. Je nach Zusammenhang bzw. Verwendung werden diese als Stopp-Set, Optimierungs-Set oder als Validation-Set bezeichnet. Diese können ebenfalls mit der Funktion train_test_split erzeugt werden.\n",
    "\n",
    "Für den Anfang wollen wir lediglich einen weiteren Datensatz, um Overfitting zu vermeiden. Dieser wird später bei jedem Trainingsschritt parallel beobachtet. Das ursprüngliche Training-Set (zunächst als temp bezeichnet) teilen wir dazu erneut auf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata_reg = mydata[['Ca','Cl','DOC','LF_mS_m','K','Mg','Na','pH','SO4','SiO2','U','PSM','Aquifer_kz','landuse_num','NO3']]\n",
    "mydata_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "temp, test = train_test_split(mydata, test_size=0.2, random_state=27)\n",
    "\n",
    "train, val = train_test_split(temp, test_size=0.1, random_state=27)\n",
    "\n",
    "print('Größe Trainings-Set: ', train.shape)\n",
    "print('Größe Validation-Set: ', val.shape)\n",
    "print('Größe Test-Set: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe03d60",
   "metadata": {},
   "source": [
    "### Skalieren\n",
    "\n",
    "Für neuronale Netze müssen die Eingangsdaten skaliert werden. Je nach Datensatz und Fragestellung kann man die Daten entweder Normieren oder Standardisieren (siehe Übung Termin 2). Wir wählen hier die Standardisierung, die für Regressionsproblem gut geeignet ist.\n",
    "\n",
    "Der Scaler wird zunächst an die Daten des Trainings-Set gefittet. Anschließend werden sowohl Trainings-Set,  Validation-Set und Test-Set transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ce098",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "val = scaler.transform(val)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6f49d",
   "metadata": {},
   "source": [
    "### Aufteilen der Daten in Input- und Zieldaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[:, :-1] # for all but last column --> Input variables\n",
    "y_train = train[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_val = val[:, :-1] # for all but last column --> Input variables\n",
    "y_val = val[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_test = test[:, :-1] # for all but last column --> Input variables\n",
    "y_test = test[:, -1] # for last column --> Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c914261",
   "metadata": {},
   "source": [
    "### Aufbau und Training des ANN\n",
    "\n",
    "Wir importieren zunächst weitere nötige Pakete, und lesen dann die Anzahl der Input-features aus. Anschließend erstellen wir ein einfaches ANN mit einer Eingabeschicht (welche automatisch auf Basis der Anzahl der Input-Features erzeugt wird), einer verdeckten Schicht mit 5 Neuronen und einer Ausgabeschicht mit einem Neuron für unseren Zielwert (Nitrat).\n",
    "\n",
    "Weitere Hyperparameter des Modells (neben der Anzahl der Schichten und Neuronen) sind die Initialisierungsfunktion, die Aktivierungsfunktion, der Optimizer und die Batch-Size (Anzahl der Inputs, die dem Netz pro Epoche auf einmal präsentiert werden). Diese werden zunächst relativ wahllos gewählt. Als Aktivierungsfunktion verwenden wir für die verdeckte Schicht \"ReLu\", was für Regressionsprobleme in der Regel gut funktioniert. Für die Ausgabeschicht wird keine Aktivierungsfunktion verwendet, da es sich um ein Regressionsproblem handelt und wir daran interessiert sind, numerische Werte direkt ohne Transformation vorherzusagen.\n",
    "\n",
    "Der Adam-Optimizer gilt ebenfalls für viele Probleme als Methode der Wahl. Bei der Batch-Size findet man häufig Anfangswerte von 16, 32 oder 64. Alle Hyperparameter können später \"optimiert\" werden.\n",
    "\n",
    "Als loss-Funktion kommt für die Regression der mean squared error zum Einsatz. Die Anzahl der Epochen erhöhen wir zunächst auf 100, da eine Regression ein komplexeres Problem darstellt, das evtl. mehr Training benötigt.\n",
    "\n",
    "Nach dem eigentlichen Training (model.fit) führen wir eine Vorhersage mit den validation-Inputs aus, um einen Hinweis dafür zu bekommen, wie gut das Modell ist. Dazu vergleichen wir die Vorhersage mit den bekannten Zielwerten und berechnen den MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "from matplotlib import pyplot\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# set initializer\n",
    "kernel_initializer = initializers.glorot_uniform()\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', kernel_initializer=kernel_initializer, input_shape=(n_features,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=3, validation_data=(X_val,y_val))\n",
    "\n",
    "# predict test set\n",
    "yhat = model.predict(X_val)\n",
    "\n",
    "# evaluate predictions\n",
    "score = mean_squared_error(y_val, yhat)\n",
    "print('MSE: %.3f' % score)\n",
    "\n",
    "# plot learning curves\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('MSE')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fc240",
   "metadata": {},
   "source": [
    "Wir sehen, dass ab etwa 60 Epochen ein Overfitting eintritt. Da dies aber sehr stark von den tatsächlichen Eingangsdaten sowie der Initialisierung abhängt, ist es besser, keine absolute Zahl festzulegen, sondern die Technik des \"Early Stopping\" anzuwenden. Dabei werden nach jeder Epoche die Gewichte abgespeichert und sobald sich der Fehler im Validierungsdatensatz für eine bestimmte Anzahl von Epochen nicht mehr verbessert, bricht das Training ab (Early Stopping) und springt zu den gespeicherten Gewichten des \"besten\" Durchlaufs zurück.\n",
    "\n",
    "Im Folgenden der Code von oben mit Early Stopping. Dabei empfiehlt es sich, die Anzahl der Epochen etwas höher zu setzen, damit das Early Stopping auch greifen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# set initializer\n",
    "kernel_initializer = initializers.glorot_uniform()\n",
    "\n",
    "# patient early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', kernel_initializer=kernel_initializer, input_shape=(n_features,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=1, validation_data=(X_val,y_val),\n",
    "                    callbacks=[es])\n",
    "\n",
    "# predict test set\n",
    "yhat = model.predict(X_val)\n",
    "\n",
    "# evaluate predictions\n",
    "score = mean_squared_error(y_val, yhat)\n",
    "print('MSE: %.3f' % score)\n",
    "\n",
    "# plot learning curves\n",
    "pyplot.title('Learning Curves')\n",
    "pyplot.xlabel('Epoch')\n",
    "pyplot.ylabel('MSE')\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74e677",
   "metadata": {},
   "source": [
    "### kfold Crossvalidation\n",
    "Um nun zu testen, wie robust die Vorhersage ist, wiederholen wir das Ganze mit einer kfold crossvalidation, d.h. wir splitten unsere Daten in jeweils 90% Trainings- und 10% Validierungsdaten. Dafür müssen wir die Reihenfolge des Datensplitting und der Skalierung ggü. oben etwas umstellen. Das Training und die Validation werden anschließend in einer Schleife ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "temp, test = train_test_split(mydata_reg, test_size=0.2, random_state=27)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(temp)\n",
    "\n",
    "temp = scaler.transform(temp)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "X, y = temp[:, :-1], temp[:, -1]\n",
    "\n",
    "X_test, y_test = test[:, :-1], test[:, -1]\n",
    "\n",
    "\n",
    "# prepare cross validation\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "kfold = KFold(10, shuffle=True, random_state=27)\n",
    "\n",
    "# enumerate splits\n",
    "scores = list()\n",
    "\n",
    "\n",
    "for train_ix, val_ix in kfold.split(X, y):\n",
    "    # split data\n",
    "    X_train, X_val, y_train, y_val = X[train_ix], X[val_ix], y[train_ix], y[val_ix]\n",
    "    # determine the number of input features\n",
    "    n_features = X.shape[1]\n",
    "    # set initializer\n",
    "    kernel_initializer = initializers.glorot_uniform\n",
    "    \n",
    "    # patient early stopping\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5, restore_best_weights=True)\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', kernel_initializer=kernel_initializer, input_shape=(n_features,)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=3, validation_data=(X_val,y_val),\n",
    "                       callbacks=[es])\n",
    "\n",
    "    # predict test set\n",
    "    yhat = model.predict(X_val)\n",
    "    score = mean_squared_error(y_val, yhat)\n",
    "    print('MSE: %.3f' % score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    # plot learning curves\n",
    "    pyplot.title('Learning Curves')\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.ylabel('MSE')\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='val')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# summarize all scores\n",
    "print('Mean MSE: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da14fdc",
   "metadata": {},
   "source": [
    "Wir sehen, dass sich sowohl der Verlauf der Lernkurven als auch die Performance (also der MSE) der einzelnen Durchläufe zum Teil stark unterscheiden kann, was zum Einen von den unterschiedlichen Initialisierungen und zum Anderem von der Aufteilung des Datensatzes abhängt.\n",
    "\n",
    "Auch bei einer zufälligen Aufteilung kann es sein, dass \"zufällig\" mehr schwierig vorherzusagende Werte im Validierungsatz landen. Dies sind vor allem Werte, die in der Verteilung der Trainingsdaten seltener vorkommen. Im Beispiel von Nitrat wären das hohe Messwerte. Daher ist es wichtig, mit einer kfold-Crossvalidation die mittlere Performance zu betrachten.\n",
    "\n",
    "Dem Problem der Initialisierung begegnet man in der Regel damit, dass man viele Modelle mit unterschiedlichen Initialisierungen rechnet (also ein Ensemble bildet) und dann jeweils die mittlere Vorhersage aller Modelle betrachtet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bdb80",
   "metadata": {},
   "source": [
    "### Optimierung von Hyperparametern (der \"einfache\" Weg)\n",
    "\n",
    "Ein einfacher Weg, Hyperparameter wie die Batchsize, Lernregel usw. zu optimieren, ist es, einfach alle möglichen Kombinationen durchzuprobieren. Dies wird als \"grid search\" bezeichnet. Der Vorteil ist, dass man auf jeden Fall die beste Variante finden wird, der Nachteil ist, dass bei einer größeren Anzahl von zu optimierenden Hyperparametern und deren jeweils mögliche Werte(-bereiche) die Anzahl der möglichen Kombinationen schnell so groß wird, dass eine grid search zeitlich gar nicht mehr möglich ist. Es gibt aber noch weitere, effizientere Methoden, dazu in einer späteren Vorlesung mehr.\n",
    "\n",
    "Im Folgenden führen wir eine kleine grid search durch, wobei wir zwei weitere Optimizer, drei verschiedene Epochen-Anzahlen und drei verschiedene Batch-Sizes testen (also 27 mögliche Kombinationen). Schon das könnte je nach Rechner ganz schön lange dauern. Falls es zu lange dauert, können wir die Anzahl der zu optimierenden Parameter reduzieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011096fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "def baseline_model(optimizer='adam'):\n",
    "   \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(5, activation='relu', kernel_initializer='glorot_uniform', input_shape=(n_features,)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# import, split and scale data\n",
    "temp, test = train_test_split(mydata_reg, test_size=0.2, random_state=27)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(temp)\n",
    "\n",
    "temp = scaler.transform(temp)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "X, y = temp[:, :-1], temp[:, -1]\n",
    "\n",
    "    \n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=16, verbose=1)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam', 'SGD']\n",
    "epochs = [30, 50, 70]\n",
    "batches = [8, 16, 32]\n",
    "\n",
    "param_grid = dict(optimizer=optimizers,\n",
    "epochs=epochs,\n",
    "batch_size=batches)\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932673b",
   "metadata": {},
   "source": [
    "### Vorhersage der Test-Daten mit dem besten Modell\n",
    "\n",
    "Jetzt wo wir wissen, was die beste Hyperparameter-Kombination ist, bauen wir das Modell nochmals neu auf, trainieren es mit dem kompletten Trainingsdatensatz (inklusive der Valididierungs-Daten) und sagen damit die Nitrat-Werte unseres Test-Datensatzes voraus. Ggf. weicht die \"beste\" Hyperparameter-Kombination von der hier gewählten ab.\n",
    "\n",
    "Weiterhin nehmen wir die Skalierung getrennt nach Input- und Target-Daten vor, was später eine inverse Transformation erleichtert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf406de",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(mydata_reg, test_size=0.2, random_state=27)\n",
    "\n",
    "# Input- and Target-Data\n",
    "X_train = train.values[:, :-1] # for all but last column --> Input variables\n",
    "y_train = train.values[:, -1] # for last column --> Target variable\n",
    "\n",
    "X_test = test.values[:, :-1] # for all but last column --> Input variables\n",
    "y_test = test.values[:, -1] # for last column --> Target variable\n",
    "\n",
    "# reshape 1d arrays to 2d arrays\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "# Scale data\n",
    "scaler_input, scaler_target = StandardScaler(), StandardScaler()\n",
    "scaler_input.fit(X_train)\n",
    "scaler_target.fit(y_train)\n",
    "\n",
    "X_train = scaler_input.transform(X_train)\n",
    "y_train = scaler_target.transform(y_train)\n",
    "\n",
    "X_test = scaler_input.transform(X_test)\n",
    "y_test = scaler_target.transform(y_test)\n",
    "\n",
    "\n",
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation='relu', kernel_initializer='glorot_uniform', input_shape=(n_features,)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='SGD', loss='mean_squared_error')\n",
    "\n",
    "# fit/train the model\n",
    "history = model.fit(X_train, y_train, epochs=70, batch_size=32, verbose=1)\n",
    "\n",
    "# predict Test data\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "# Scores\n",
    "score = mean_squared_error(y_test, yhat)\n",
    "print('MSE: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d4d0f",
   "metadata": {},
   "source": [
    "### Vergleich vorhergesagte und tatsächliche Daten\n",
    "\n",
    "Um die vorhergesagten und tatsächlichen Daten zu vergleichen, können wir nun weitere Fehlermaße berechnen (z.B. MSE, RMSE, R²) sowie die Daten in einem Scatterplot darstellen. Dazu re-transformieren wir die Zieldaten zunächst wieder auf die ursprünglichen Werte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d138fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# inverse transform\n",
    "y_test = scaler_target.inverse_transform(y_test)\n",
    "yhat = scaler_target.inverse_transform(yhat)\n",
    "\n",
    "# Scores\n",
    "mse = mean_squared_error(y_test, yhat)\n",
    "r2 = r2_score(y_test, yhat)\n",
    "print('MSE: %.3f' % mse)\n",
    "print('RMSE: %.3f' % np.sqrt(mse))\n",
    "print('R_squared: %.3f' % r2)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_test, yhat)\n",
    "plt.plot(y_test, y_test, color = 'red', label = 'x=y')\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e5525",
   "metadata": {},
   "source": [
    "Naja, nicht ganz perfekt :-) Vor allem die höheren Werte scheinen schwieriger vorherzusagen zu sein. Das liegt daran, dass diese auch im Trainingsdatensatz seltener vorkommen (ein bekanntes Problem, was als \"imbalanced regression/classification\" bezeichnet wird).\n",
    "\n",
    "Eine weitere Optimierung könnte man durch folgende Punkte erreichen:\n",
    "\n",
    "* Die Aquiferkennzahl ist eigentlich eine kategorische Variable, wurde bisher aber ordinal encodiert. Hier könnte man ein One-Hot- oder Target-Encoding testen.\n",
    "* Vermutlich korrelieren einige der Input-Variablen recht stark miteinander. Durch eine vorgeschaltete PCA könnten sich die Vorhersagen ebenfalls verbessern.\n",
    "* Die Hyperparameter könnten noch besser optimiert werden, d.h. mehr Optionen für die bereits getesteten Hyperparameter oder weitere wie die Aktivierungsfunktion, Initialisierung und Lernrate.\n",
    "* Man könnte die Anzahl der Neuronen und die Anzahl der Schichten verändern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ad91d",
   "metadata": {},
   "source": [
    "## Übungsaufgabe 1\n",
    "\n",
    "Verändern Sie die Anzahl der Neuronen in der verdeckten Schicht! Sie können dies ebenfalls mit einer GridSearchCV machen, in dem Sie einen Parameter \"neurons\" im dictionary param_grid (s.o.) definieren und für diesen verschiedene Anzahlen als Optionen vorgeben. Damit die GridSearch nicht zu lange dauert, sollten sie einen oder zwei der anderen Optimierungsparameter dafür rausnehmen. Experimentieren Sie außerdem noch mit einer zweiten verdeckten Schicht und optimieren Sie in dieser ebenfalls die Anzahl der Neuronen.\n",
    "Achtung: Bei der Definition des baseline_model müssen sie für Parameter, die später über das param_dict definiert werden, einen Anfangswert eingeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def baseline_model(optimizer='adam', neurons=5):\n",
    "   \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', kernel_initializer='glorot_uniform', input_shape=(n_features,)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# import, split and scale data\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f7c789",
   "metadata": {},
   "source": [
    "## Übungsaufgabe 2\n",
    "\n",
    "Da sich mit einer veränderten Zahl der Neuronen und/oder Schichten wiederum andere Hyperparameter, vor allem die Anzahl der Epochen und der Batch-Size als günstiger erweisen könnten, ist es am besten, möglichst viele Hyperparameter gleichzeitig zu optimieren. Hier stößt die GridSearchCV aber schnell an ihre Grenzen. Eine weitere Alternative ist die RandomizedSearchCV, bei der eine definierte Anzahl an zufällig gewählten Kombination aus dem vorgegebenen Parameter-Space getestet werden. Hierbei ist zwar nicht garantiert, dass die absolut beste Kombination gefunden wird, jedoch liefert es meistens bei einem geringeren Rechenaufwand schon ganz brauchbare Hinweise, welche Kombinationen gut funktionieren und welche nicht.\n",
    "\n",
    "Achtung: beim Befehl RandomizedSearchCV wird der Parameter \"param_grid\" durch \"param_distributions\" ersetzt. Ansonsten ist die Verwendung exakt gleich wie bei GridSearchCV. Als weitere Option kann man die Anzahl der Iterationen über \"n_iter\" festelegen. Default-Wert ist 10.\n",
    "\n",
    "Optimieren Sie nun mehrere Hyperparameter gleichzeitig! Für den optimizer können Sie z.B. zusätzlich die Optionen 'Adagrad', 'Adadelta', 'Adamax' und 'Nadam' testen. Für die Aktivierungsfunktion kommen z.B. 'softmax', 'softplus', 'softsign', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear' in Frage und für die Initialisierung 'uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'. Je mehr Parameter und Optionen Sie gleichzeitig verwenden, desto höher sollten Sie den Wert für n_iter setzen, damit eine vernünftige Anzahl der möglichen Kombinationen ausprobiert wird. Als Daumenwert sollte dies mindestens etwa 0,1 * die Anzahl aller möglichen Kombinationen sein!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bee958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def baseline_model(optimizer='adam', neurons=5, activation='relu'):\n",
    "   \n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation=activation, kernel_initializer='glorot_uniform', input_shape=(n_features,)))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# import, split and scale data\n",
    "# ...\n",
    "\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, scoring='neg_mean_squared_error', n_iter=50)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bca562",
   "metadata": {},
   "source": [
    "Wenn Sie eine bessere Parameterkombination als in der Vorlesung verwendet finden, so schreiben Sie den Algorithmus zur Vorhersage der Test-Daten mit dem besten Modell entsprechend um und sagen Sie dann mit diesem wiederum die Test-Daten voraus. Wie haben sich hier die Ergebnisse verändert?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimized model for prediction of test data\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88df38c",
   "metadata": {},
   "source": [
    "## Übungsaufgabe 3\n",
    "\n",
    "Wenden Sie die gelernten Methoden des Kfold Crossvalidation und der Hyperparameter-Optimierung auch auf die Binär-Klassifikation an und schauen Sie, ob Sie die Performance des Random Forest aus Termin 4 übertreffen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff49dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimized model for prediction of test data\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b8ed7",
   "metadata": {},
   "source": [
    "## Übungsaufgabe 4\n",
    "\n",
    "Wenden Sie nun die hier gelernten Methoden zur Regression mit einem MLP auf den Beton-Datensatz aus Termin 4 (Concrete_Data.csv) an! Wie sind die Ergebnisse, die Sie erzielen können im Vergleich zum in Termin 4 angewandten Random-Forest Methode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29cb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5facce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimized model for prediction of test data\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecb96c",
   "metadata": {},
   "source": [
    "## Übungsaufgabe 5\n",
    "\n",
    "Testen Sie, ob Sie für den Grundwasserqualitätsdatensatz die Vorhersage von Nitrat verbessern können, wenn Sie eine PCA vorschalten und/oder den Aquifer anders codieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338a63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
