{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97672b46",
   "metadata": {},
   "source": [
    "# Geodatenanalyse 2: Termin 8 - Zeitreihenvorhersage und rekurrente neuronale Netze\n",
    "\n",
    "## Zeitreihenvorhersage mit 1D-CNN\n",
    "\n",
    "Ziel der Übung: Grundwasserstand simulieren für einen Zeitraum von 4 Jahren basierend auf meteorologischen Inputdaten (z.B. Niederschlag). \n",
    "\n",
    "Wir können KNN verwenden um den Zusammenhang zwischen relevanten meteorologischen Parametern wie Niederschlag oder Temperatur und dem Grundwasserstand herzustellen, auch ohne die direketen Prozesse (Grundwasserneubildung, Evapotranspiration, ...) zu definieren. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f65ca",
   "metadata": {},
   "source": [
    "### set random seed\n",
    "Um ein reproduzierbares Ergebnis zu erhalten definieren wir zunächst die Startpunkte (seeds) der Zufallszahlengeneratoren von numpy und tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed for reproducability\n",
    "from numpy.random import seed\n",
    "seed(347824) # this is a randomly chosen number\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(63494) # this is also a randomly chosen number\n",
    "\n",
    "print(\"seeds are set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38376dea",
   "metadata": {},
   "source": [
    "### load data\n",
    "Die (wöchentlich) Grundwasserdaten befinden sich in der Datei: **GW-Data.csv**  \n",
    "Die meteorologischen Inputdaten in der Datei: **Climate-Data.csv**\n",
    "\n",
    "Wir laden die Dateien als pandas Dataframe mit einem datetimeindex ein und führen beide Dateien zusammen. Ein weiteres Preprocessing ist in diesem Fall nicht nötig, da die Daten schon auf Datenlücken/Fehlwerte etc. untersucht und bereinigt wurden. Auch decken alle Daten den gleichen Zeitraum ab und verfügen über ein identisches Sampling Intervall (1 Woche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "GWData = pd.read_csv('./GW-Data.csv',\n",
    "                     parse_dates=['Date'], # hiermit werden Zeitangaben als Datum erkannt\n",
    "                     index_col=0, # definiert welche Spalte der gelesenen Daten als Index für den DataFrame dienen soll\n",
    "                     dayfirst = True, # definiert das Datumsformat \n",
    "                     decimal = '.', sep=',') # Angaben welche Trennzeichen in der Datei verwendet werden\n",
    "\n",
    "ClimateData = pd.read_csv('Climate-Data.csv', \n",
    "                          parse_dates=['Date'],index_col=0,dayfirst = True,\n",
    "                          decimal = '.', sep=',')\n",
    "\n",
    "# Beide DataFrames zusammenführen\n",
    "data = pd.merge(GWData, ClimateData, how='inner', left_index = True, right_index = True) # beide DataFrames zusammenführen\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot GWL data\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(data.index,data['GWL'], 'k', label =\"GWL\", linewidth = 1.7)\n",
    "plt.title(\"GWL Data\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(b=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "#inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc95ce",
   "metadata": {},
   "source": [
    "Der DataFrame setzt sich nun zusammen aus der Indexspalte **Date**, dem Grundwasserstand **GWL**, Niederschlag **P**, relativer Feuchte **rH**, Temperatur **T** und einem geglätteten Temperatursignal **Tsin** (Sinuskurve an T gefittet).  \n",
    "\n",
    "Parameter **rH** und **Tsin** werden zunächst nicht benötigt (*drop*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data = data.drop(columns='rH')\n",
    "    data = data.drop(columns='Tsin')\n",
    "    print(\"dropped\")\n",
    "except: \n",
    "    print(\"already dropped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678b6ae",
   "metadata": {},
   "source": [
    "die *try ... except* - Syntax verhindert einen Fehler, wenn die Variablen nicht vorhanden sind oder schon entfernt wurden. In unserem Fall also z.B. wenn die Zelle im notebook bereits ausgeführt wurde.  \n",
    "\n",
    "Wie immer führen viele Wege zum Ziel. Man könnte auch schreiben\n",
    "- data.drop(columns=['rH','Tsin'], inplace = True)\n",
    "- data = data.drop(['rH','Tsin'], axis = 1)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a92599",
   "metadata": {},
   "source": [
    "### we are using sequence models!\n",
    "   \n",
    "   \n",
    "\n",
    "![Inputsequenz](./Inputsequence.png)\n",
    "\n",
    "Wir müssen also zunächst **m** festlegen und die Daten so umwandeln, dass einem GWL-Wert immer **m** P und T Werte zugeordnet sind. Man spricht bei **m** auch von der **input sequence length**. Wir bezeichen diesen Parameter im Folgenden als **n_steps_in** (*Anzahl der Input Zeitschritte*).\n",
    "\n",
    "Hierfür definieren wir eine Funktion die genau das für uns erledigt und die wir immer wieder verwenden können: *make_sequences*    \n",
    "\n",
    "Diese können wir nachher mit folgender Zeile aufrufen:  \n",
    "*X,Y = make_sequences(data, n_steps_in)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e536a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data, n_steps_in):\n",
    "#     \"\"\"\n",
    "#     data: numpy array with target (Y) in first column and model inputs in following columns\n",
    "#     n_steps_in: number that defines the sequence length\n",
    "    \n",
    "#     Output:\n",
    "#     X: sequenced model input data\n",
    "#     Y: sequenced model target data\n",
    "    \n",
    "#     function modified after: machinelearningmastery.com\n",
    "#     \"\"\"\n",
    "    \n",
    "    #sequence the data\n",
    "    X, Y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(data)):\n",
    "        # find the end of this pattern\n",
    "        end_idx = i + n_steps_in\n",
    "        # check if we are beyond the dataset\n",
    "        if end_idx >= len(data):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = data[i:end_idx, 1:]\n",
    "        seq_y = data[end_idx, 0]\n",
    "        X.append(seq_x)\n",
    "        Y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b57184",
   "metadata": {},
   "source": [
    "### split data\n",
    "\n",
    "Bevor wir die Daten in Sequenzen umwandeln, müssen wir aber zunächst noch den bekannten Train-Test-Split durchführen und die Daten anschließend skalieren. Wichtig, shuffling ist für Zeitreihen nicht erlaubt, wir teilen die Zeitreihe einfach in zusammenhängende Abschnitte auf. Hierfür können die bereits eingeführten Funktionen verwendet werden **ABER** besonders bei Zeitreihen bietet es sich häufig an, die Zeitpunkte an denen geteilt wird, selbst festzulegen. Es ist einfach häufig sinnvoller mit Jahren als mit prozentualen Anteilen einer Zeitreihe zu arbeiten.\n",
    "\n",
    "Daher teilen wir nun die Zeitreihe in drei Abschnitte auf:  \n",
    "**Training, Early Stopping (Validation) und Testing**  \n",
    "\n",
    "Zum Testen sollen die Jahre 2012 bis einschließlich 2015 (also 4 Jahre) verwendet werden, für Early Stopping 2 Jahre (2010 - 2011), fürs Training die restlichen Daten davor.\n",
    "\n",
    "Hierfür können wir den Datumsindex des Dataframe verwenden. Zunächst definieren wir uns jedoch ein Datum, zu welchem Die Daten geteilt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_startdate = pd.to_datetime('01012010', format='%d%m%Y')\n",
    "print(\"Start Stopset\")\n",
    "print(val_startdate)\n",
    "\n",
    "test_startdate = pd.to_datetime('01012012', format='%d%m%Y')\n",
    "print(\"\\nStart Testset\") #\\n steht für einen Zeilenumbruch\n",
    "print(test_startdate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e9274",
   "metadata": {},
   "source": [
    "#### Überlappung beim split notwendig!\n",
    "\n",
    "Beim Teilen gilt es die eben angesprochene Sequenzlänge *n_steps_in* zu beachten, denn wenn die Zeitreihe zum 1.1.2012 geteilt werden sollen, so werden aus dem Jahr 2012 noch *n_steps_in* Zeitschritte benötigt um den ersten Wert in 2012 berechnen zu können. Die Zeiträume überlappen sich also in Bezug auf die Inputdaten *X* ( *_ext* = extended). \n",
    "\n",
    "Beispiel: Tagesdaten und beliebiges Modell mit *n_steps_in = 4*:\n",
    "\n",
    "       28.12.2011   X        Beginn Überlapp  \n",
    "       29.12.2011   X  X\n",
    "       30.12.2011   X  X  X\n",
    "       31.12.2011   X  X  X\n",
    "       1.1.2012     Y  X  X  Beginn Testzeitraum\n",
    "       2.1.2012        Y  X\n",
    "       3.1.2012           Y\n",
    "       [...]\n",
    "\n",
    "Für den Trainigsdatensatz gilt entsprechend, dass der erste simulierte Wert erst nach *n_steps_in* Zeitschritten berechnet, wird. Die bis dahin ungenutzten Zielwerte werden verworfen. Bei großen Sequenzlängen kann hierdurch der Trainingszeitraum also u.U. wesentlich verkleinert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63c6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_in = 30\n",
    "\n",
    "TrainingData = data[(data.index < val_startdate)] \n",
    "StopData = data[(data.index >= val_startdate) & (data.index < test_startdate)]\n",
    "TestData = data[(data.index >= test_startdate)] \n",
    "\n",
    "# extend Stop+Testdata to be able to fill sequence later                                              \n",
    "StopData_ext = pd.concat([TrainingData.iloc[-n_steps_in:], StopData], axis=0) #takes last steps of TrainingData and combines it with StopData\n",
    "TestData_ext = pd.concat([StopData.iloc[-n_steps_in:], TestData], axis=0) #takes last steps of StopData and combines it with TestData\n",
    "\n",
    "print(StopData_ext) #beginnt nun n_steps_in Zeitschritte vor stop_startdate\n",
    "print(TestData_ext) #beginnt nun n_steps_in Zeitschritte vor test_startdate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32165ea",
   "metadata": {},
   "source": [
    "### scale data \n",
    "Jetzt skalieren wir die Daten auf den Wertebereich [-1,1] entsprechend des Trainingdatensatzes. Wie in der letzten Übung erstellen wir noch einen scaler der nur die Zielvariable scaled um nachher einfacher retransformieren zu können (*scaler_gwl*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#fit scaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(TrainingData)\n",
    "\n",
    "scaler_gwl = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_gwl.fit(pd.DataFrame(TrainingData['GWL']))\n",
    "\n",
    "#scale data\n",
    "TrainingData_n = scaler.transform(TrainingData)\n",
    "StopData_ext_n = scaler.transform(StopData_ext)\n",
    "TestData_ext_n = scaler.transform(TestData_ext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934a815",
   "metadata": {},
   "source": [
    "Nun nutzen wir die Sequenz Funktion von oben, aus einem 2D (Länge Datensatz, Anzahl Parameter) wird damit ein 3D Datensatz mit den Dimensionen: (Länge Datensatz, n_steps_in, Anzahl Parameter).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = make_sequences(np.asarray(TrainingData_n), n_steps_in)\n",
    "X_stop,Y_stop = make_sequences(np.asarray(StopData_ext_n), n_steps_in)\n",
    "X_test,Y_test = make_sequences(np.asarray(TestData_ext_n), n_steps_in)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "\n",
    "print(X_stop.shape)\n",
    "print(Y_stop.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78218b",
   "metadata": {},
   "source": [
    "Diese Daten können wir nun direkt in das Modell eingeben. Dieses definieren wir im Folgenden. Wir starten mit einem **1D-CNN** und nutzen hierfür die Keras API in Tensorflow (*tensorflow.keras*):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12052240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "ini=0\n",
    "seed(ini+872527)\n",
    "tf.random.set_seed(ini+87747)\n",
    "    \n",
    "#Define Input Shape\n",
    "inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2]))\n",
    "\n",
    "#start with a 1D-Conv Layer\n",
    "cnn = tf.keras.layers.Conv1D(filters=256,\n",
    "                             kernel_size=3,\n",
    "                             activation='relu',\n",
    "                             padding='same')(inp)\n",
    "#add Pooling Layer\n",
    "cnn = tf.keras.layers.MaxPool1D(padding='same')(cnn) #consolidates the feature_map information\n",
    "\n",
    "#reshape data for next layer\n",
    "cnn = tf.keras.layers.Flatten()(cnn) # flattens the data\n",
    "\n",
    "#add a dense layer\n",
    "cnn = tf.keras.layers.Dense(30, activation='relu')(cnn)\n",
    "\n",
    "#add output neuron\n",
    "output1 = tf.keras.layers.Dense(1, activation='linear')(cnn) # output neuron to get meaningful output values\n",
    "\n",
    "#tie together\n",
    "model = tf.keras.Model(inputs=inp, outputs=output1)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1E-3, epsilon=1E-3, clipnorm=True)\n",
    "  \n",
    "#compile the model\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#define early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                    batch_size=32, callbacks=[es])\n",
    "\n",
    "#plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdd27e",
   "metadata": {},
   "source": [
    "Jetzt haben wir ein fertig trainiertes Modell, das wir zur Simulation von ungesehenen Daten (TestData) verwenden können:\n",
    "\n",
    "Wir geben dafür *X_test* in das Modell und vergleichen das Ergebnis mit *Y_test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = model.predict(X_test)\n",
    "\n",
    "#rescale simulated and observed values\n",
    "sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "obs = scaler_gwl.inverse_transform(Y_test.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80ab9",
   "metadata": {},
   "source": [
    "Wir berechnen noch einige Fehlerwerte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Nash-Sutcliffe-Efficiency: NSE\n",
    "NSE = 1 - ((np.sum((sim-obs) ** 2)) / (np.sum((obs - np.mean(obs)) ** 2)))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "RMSE =  np.sqrt(np.mean((sim-obs) ** 2))\n",
    "\n",
    "#Pearson r\n",
    "r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "r = r[0] #r\n",
    "\n",
    "#Bias\n",
    "Bias = np.mean(sim-obs)\n",
    "\n",
    "scores = pd.DataFrame(np.array([[NSE, RMSE, r, Bias]]),\n",
    "                      columns=['NSE','RMSE','r','Bias'])\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ba7f4",
   "metadata": {},
   "source": [
    "Ergebnis plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb817d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(TestData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "plt.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "plt.title(\"Simulation of TestData\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,bbox_to_anchor=(1.12, 1),loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(b=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90cfdf",
   "metadata": {},
   "source": [
    "### Aufgabe 1: CNN\n",
    "\n",
    "Macht euch mit dem Modell vertraut und verändert einmal folgende Parameter:  \n",
    "- Anzahl der Inputparameter\n",
    "- n_steps_in\n",
    "- Anzahl der Filter im 1D-Conv Layer\n",
    "- Training batch size\n",
    "- Epochenanzahl und Early Stopping Patience\n",
    "- Welchen Zweck könnte der Inputparameter Tsin haben?\n",
    "\n",
    "Wie ändert sich die Vorhersagegüte? Was für einenen NSE könnt ihr erreichen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6241cc",
   "metadata": {},
   "source": [
    "### Aufgabe 2: LSTM\n",
    "Ersetzt das 1D-CNN Modell durch ein LSTM. \n",
    "Hinweis: Es sind nur Änderungen in der Zelle in der das Modell definiert wird notwendig. \n",
    "\n",
    "orientiert euch an folgender Struktur:\n",
    "\n",
    "![LSTM](./LSTM.png)\n",
    "\n",
    "hier könnt ihr die Syntax nachlesen:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "Einfach Googeln hilft aber auch meist weiter.\n",
    "\n",
    "Welches des beiden Modelle erzielt eine bessere Performance? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d7f53",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set seed\n",
    "ini=0\n",
    "seed(ini+872527)\n",
    "tf.random.set_seed(ini+87747)\n",
    "    \n",
    "#Define Input Shape\n",
    "inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2]))\n",
    "\n",
    "# ***************Hier muss das neue Modell und entsprechende Zeilen von oben rein#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **************\n",
    "#define early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                    batch_size=32, callbacks=[es])\n",
    "\n",
    "#plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "#simulate testset\n",
    "sim = model.predict(X_test)\n",
    "\n",
    "#rescale simulated and observed values\n",
    "sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "obs = scaler_gwl.inverse_transform(Y_test.reshape(-1,1))\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Nash-Sutcliffe-Efficiency: NSE\n",
    "NSE = 1 - ((np.sum((sim-obs) ** 2)) / (np.sum((obs - np.mean(obs)) ** 2)))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "RMSE =  np.sqrt(np.mean((sim-obs) ** 2))\n",
    "\n",
    "#Pearson r\n",
    "r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "r = r[0] #r\n",
    "\n",
    "#Bias\n",
    "Bias = np.mean(sim-obs)\n",
    "\n",
    "scores = pd.DataFrame(np.array([[NSE, RMSE, r, Bias]]),\n",
    "                      columns=['NSE','RMSE','r','Bias'])\n",
    "print(scores)\n",
    "\n",
    "#plot Test data and simulation\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(TestData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "plt.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "plt.title(\"Simulation of TestData\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,bbox_to_anchor=(1.18, 1),loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(b=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f4dea",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Regularisierungstechniken - robuste Modelle\n",
    "\n",
    "Overfitting und Robustness sind zentrale Punkte für das Training von ANNs. Neben einem klassischen Early Stopping gegen Overfitting gibt es noch weitere Ansätze.\n",
    "\n",
    "#### Batch-Normalization Layer\n",
    "Ihr kennt bereits das Vanishing Gradient Problem. BatchNorm Layer verhindern jetzt im Prinzip das Gegenteil, nämlich **Exploding Gradients**. Im Grunde wird dafür lediglich der Output des vorhergehenden Layers normalisiert.\n",
    "Dies hat den Effekt, dass der Lernprozess stabilisiert und die Anzahl der Trainingsepochen, die zum Trainieren von Deep ANN benötigt werden, drastisch reduziert wird.\n",
    "\n",
    "Mehr (ausführliche) Infos hier: https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/  oder hier: https://towardsdatascience.com/batch-normalization-the-greatest-breakthrough-in-deep-learning-77e64909d81d\n",
    "\n",
    "#### Dropout Layer\n",
    "Durch einen Dropout Layer werden im Grunde ein definierter Anteil der möglichen Pfade im Netzwerk an dieser Stelle zufällig deaktiviert (zufällig immer andere bei jedem Durchlauf). Dadurch wird das Training verrauscht und die Nodes innerhalb einer Schicht gezwungen, besser auf Inputs zu reagieren.\n",
    "\n",
    "\n",
    "#### Gaußian-Noise Layer\n",
    "Prinzip: Den Daten wird Rauschen hinzugefügt, sodass das Model die Zusammenhänge robuster lernen kann. Wird heute nicht in dieser Übung behandelt.  \n",
    "https://keras.io/api/layers/regularization_layers/ \n",
    "\n",
    "### Aufgabe:\n",
    "\n",
    "Füge zu dem Modell aus Aufgabe 1 ODER 2 einen BatchNorm Layer und einen Dropout Layer hinzu. Experimentiere mit der Dropout Rate. Welchen Effekt haben die Layer einzeln und gleichzeitig?\n",
    "\n",
    "**Hinweis**  \n",
    "Orientiere dich bei der Platzierung fürs erste an dieser Struktur:\n",
    "![layer](./layer.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "ini=0\n",
    "seed(ini+872527)\n",
    "tf.random.set_seed(ini+87747)\n",
    "    \n",
    "#Define Input Shape\n",
    "inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2]))\n",
    "\n",
    "# ********************************\n",
    "\n",
    "# Modell hier\n",
    "\n",
    "\n",
    "\n",
    "# *********************************\n",
    "\n",
    "#define early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                    batch_size=32, callbacks=[es])\n",
    "\n",
    "#plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "#simulate testset\n",
    "sim = model.predict(X_test)\n",
    "\n",
    "#rescale simulated and observed values\n",
    "sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "obs = scaler_gwl.inverse_transform(Y_test.reshape(-1,1))\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#Nash-Sutcliffe-Efficiency: NSE\n",
    "NSE = 1 - ((np.sum((sim-obs) ** 2)) / (np.sum((obs - np.mean(obs)) ** 2)))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "RMSE =  np.sqrt(np.mean((sim-obs) ** 2))\n",
    "\n",
    "#Pearson r\n",
    "r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "r = r[0] #r\n",
    "\n",
    "#Bias\n",
    "Bias = np.mean(sim-obs)\n",
    "\n",
    "scores = pd.DataFrame(np.array([[NSE, RMSE, r, Bias]]),\n",
    "                      columns=['NSE','RMSE','r','Bias'])\n",
    "print(scores)\n",
    "\n",
    "#plot Test data and simulation\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(TestData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "plt.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "plt.title(\"Simulation of TestData\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,bbox_to_anchor=(1.18, 1),loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(b=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76de0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
