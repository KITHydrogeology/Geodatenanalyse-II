{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97672b46",
   "metadata": {},
   "source": [
    "# Geodatenanalyse 2: Bayesische Hyperparameteroptimierung und Ensemble-Modellierung\n",
    "\n",
    "Ziel der Übung: Für das Modell aus dem Termin zur Zeitreihenvorhersage von Grundwasserständen sollen Hyperparameter optimiert werden. Wir möchten die optimale die Länge der Inputsequenz sowie der Anzahl der Units im LSTM Layer optimieren. \n",
    "\n",
    "Weiterhin minimieren wir den Einfluss der Modellinitialisierung (basierend auf einer Zufallszahl) durch ein Ensemble welches mehrere definierte *random number seeds* nutzt. \n",
    "\n",
    "Zunächst nutzen wir Zellen des Notebooks vom letzten Termin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9bb28b",
   "metadata": {},
   "source": [
    "### set random seed\n",
    "Um ein reproduzierbares Ergebnis zu erhalten definieren wir zunächst die Startpunkte (seeds) der Zufallszahlengeneratoren von numpy und tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed for reproducability\n",
    "from numpy.random import seed\n",
    "seed(347824) # this is a randomly chosen number\n",
    "\n",
    "from tensorflow import random\n",
    "random.set_seed(63494) # this is also a randomly chosen number\n",
    "\n",
    "print(\"seeds are set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a963f6e",
   "metadata": {},
   "source": [
    "### load data\n",
    "Die (wöchentlich) Grundwasserdaten befinden sich in der Datei: **GW-Data.csv**  \n",
    "Die meteorologischen Inputdaten in der Datei: **Climate-Data.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "GWData = pd.read_csv('./GW-Data.csv',\n",
    "                     parse_dates=['Date'], # This is used to recognise times as dates\n",
    "                     index_col=0, # defines which column of the read data is to serve as the index for the DataFrame\n",
    "                     dayfirst = True,  #defines the date format  \n",
    "                     decimal = '.', sep=',') #Specifies which separators are used in the file\n",
    "\n",
    "ClimateData = pd.read_csv('Climate-Data.csv', \n",
    "                          parse_dates=['Date'],index_col=0,dayfirst = True,\n",
    "                          decimal = '.', sep=',')\n",
    "\n",
    "# Merge Dataframes\n",
    "data = pd.merge(GWData, ClimateData, how='inner', left_index = True, right_index = True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot GWL data\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(data.index,data['GWL'], 'k', label =\"GWL\", linewidth = 1.7)\n",
    "plt.title(\"GWL Data\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "#inspect the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fde15",
   "metadata": {},
   "source": [
    "Der DataFrame setzt sich nun zusammen aus der Indexspalte **Date**, dem Grundwasserstand **GWL**, Niederschlag **P**, relativer Feuchte **rH**, Temperatur **T** und einem geglätteten Temperatursignal **Tsin** (Sinuskurve an T gefittet).  \n",
    "\n",
    "Parameter **rH** und **Tsin** werden zunächst nicht benötigt (*drop*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ab7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    data = data.drop(columns=['rH','Tsin'])\n",
    "    print(\"dropped\")\n",
    "except: \n",
    "    print(\"already dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b52833e",
   "metadata": {},
   "source": [
    "we are using sequence models!\n",
    "   \n",
    "Wir definieren wir eine Funktion die genau das für uns erledigt und die wir immer wieder verwenden können: *make_sequences*    \n",
    "\n",
    "Diese können wir nachher mit folgender Zeile aufrufen:  \n",
    "*X,Y = make_sequences(data, n_steps_in)*\n",
    "\n",
    "Inhaltlich passiert folgendes:\n",
    "\n",
    "Aus einer Zeitreihe aus Inputdaten X und Zieldaten Y:  \n",
    "    \n",
    "    XXXXXXXXXXXXXXX  \n",
    "    YYYYYYYYYYYYYYY  \n",
    "\n",
    "werden kleine Schnipsel die jeder Inputsequenz der Länge *n_steps_in* (hier = 4) ein Zielwert zuordnet\n",
    "\n",
    "    XXXXY\n",
    "     XXXXY\n",
    "      XXXXY\n",
    "       XXXXY\n",
    "        XXXXY\n",
    "         XXXXY\n",
    "          XXXXY\n",
    "          ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(data, n_steps_in):\n",
    "#     \"\"\"\n",
    "#     data: numpy array with target (Y) in first column and model inputs in following columns\n",
    "#     n_steps_in: number that defines the sequence length\n",
    "    \n",
    "#     Output:\n",
    "#     X: sequenced model input data\n",
    "#     Y: sequenced model target data\n",
    "    \n",
    "#     function modified after: machinelearningmastery.com\n",
    "#     \"\"\"\n",
    "    \n",
    "    #sequence the data\n",
    "    X, Y = list(), list()\n",
    "    # step over the entire history one time step at a time\n",
    "    for i in range(len(data)):\n",
    "        # find the end of this pattern\n",
    "        end_idx = i + n_steps_in\n",
    "        # check if we are beyond the dataset\n",
    "        if end_idx >= len(data):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = data[i:end_idx, 1:]\n",
    "        seq_y = data[end_idx, 0]\n",
    "        X.append(seq_x)\n",
    "        Y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b57184",
   "metadata": {},
   "source": [
    "### split data (diesmal in 4! Teile)\n",
    "\n",
    "Bevor wir die Daten in Sequenzen umwandeln, müssen wir aber zunächst noch den bekannten Train-Test-Split durchführen und die Daten anschließend skalieren. Wichtig, KEIN shuffling!\n",
    "\n",
    "Da wir eine Optimierungsfunktion nutzen und data leakage ins testset vermeiden möchten, brauchen wir diesmal 4 Teile unserer Daten.\n",
    "  \n",
    "**Training, Early Stopping (Validation), Optimierungs und Testing**  \n",
    "\n",
    "Wir nutzen folgende Aufeilung:\n",
    "* Test: >= 2012  \n",
    "* Optimierung: 2011  \n",
    "* Early Stopping: 2010  \n",
    "* Training: < 2010  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7984",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_startdate = pd.to_datetime('01012010', format='%d%m%Y')\n",
    "opt_startdate = pd.to_datetime('01012011', format='%d%m%Y')\n",
    "test_startdate = pd.to_datetime('01012012', format='%d%m%Y')\n",
    "\n",
    "#print dates\n",
    "print(\"Start Stopset:\\n{}\\n\\nStart Optset:\\n{}\\n\\nStart Testset:\\n{}\".format(val_startdate,opt_startdate,test_startdate))\n",
    "\n",
    "#Here we divide the data, but do not let them overlap(!), this depends on n_steps_in, which we have not yet defined\n",
    "\n",
    "TrainingData = data[(data.index < val_startdate)] \n",
    "StopData = data[(data.index >= val_startdate) & (data.index < opt_startdate)]\n",
    "OptData = data[(data.index >= opt_startdate) & (data.index < test_startdate)]\n",
    "TestData = data[(data.index >= test_startdate)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78f0c2",
   "metadata": {},
   "source": [
    "### Bayesische Optimierung: Ab hier wird es etwas anders...   \n",
    "Die Bayesische Optimierung kann nur auf eine Funktion angewendet werden. Wie bereits angesprochen möchten wir *n_steps_in* optimieren. Alles was nach der Festlegung dieses Parameters erst durchgeführt werden kann (z.B. die Überlappung der der Daten-Sets) muss also in diese Funktion reingeschrieben werden. \n",
    "\n",
    "Mit anderen Worten, wir geben dem Optimierungsalgorithmus eine Funktion die alles tut (Daten bearbeiten, Modell aufbauen trainieren, vorhersagen, evaluieren und eine Metrik ausgeben) was wir wollen, aber in Abhängigkeit des zu optimierenden Parameters *n_steps_in*.\n",
    "\n",
    "Eine solche Funktion sieht vereinfacht so aus:\n",
    "\n",
    "```python\n",
    "def bayesopt_function(n_steps_in): \n",
    "    #hier [CODE] werden die Daten geteilt, skaliert, das Modell aufgebaut, trainiert und evaluiert etc.\n",
    "    #[...]\n",
    "    return -MSE # oder irgendeine andere Metrik zur Beurteilung des Modells\n",
    "```\n",
    "\n",
    "Die Funktion nimmt verschiedene Inputparameter, baut dann das Modell (inkl. aller dafür notwendigen Schritte) und gibt uns den negativen MSE zurück, den wir maximieren möchten (also den Fehler(MSE) minimieren).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "def bayesoptfunction(n_steps_in): \n",
    "   \n",
    "    #First we have to make sure that n_steps_in is an integer. \n",
    "    #The optimisation algorithm always outputs float numbers, we convert these into an integer:\n",
    "    \n",
    "    n_steps_in = int(n_steps_in)\n",
    "   \n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    #Now follow the familiar steps as in the last appointment for time series prediction.\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    #extend with overlap to be able to fill sequence (n_steps_in) later                                              \n",
    "    StopData_ext = pd.concat([TrainingData.iloc[-n_steps_in:], StopData], axis=0) #takes last steps of TrainingData and combines it with StopData\n",
    "    OptData_ext = pd.concat([StopData.iloc[-n_steps_in:], OptData], axis=0) #takes last steps of ValidationData and combines it with OptData\n",
    "    #TestData_ext = pd.concat([OptData.iloc[-n_steps_in:], TestData], axis=0) #takes last steps of OptData and combines it with TestData\n",
    "    #The test data is not required here in this function, instead we use the option set for evaluation.\n",
    "    \n",
    "    #create scalers and fit to data data (normalize: *_n):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(TrainingData) # fit on training data, scale all other parts afterwards\n",
    "\n",
    "    scaler_gwl = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    scaler_gwl.fit(pd.DataFrame(TrainingData['GWL'])) # fit scaler only on GWL for rescaling later\n",
    "    \n",
    "    #scale (transform) data\n",
    "    TrainingData_n = scaler.transform(TrainingData)\n",
    "    StopData_ext_n = scaler.transform(StopData_ext)\n",
    "    OptData_ext_n = scaler.transform(OptData_ext)\n",
    "    \n",
    "    #Convert data into sequences:\n",
    "    X_train,Y_train = make_sequences(np.asarray(TrainingData_n), n_steps_in)\n",
    "    X_stop,Y_stop = make_sequences(np.asarray(StopData_ext_n), n_steps_in)\n",
    "    X_opt,Y_opt = make_sequences(np.asarray(OptData_ext_n), n_steps_in)\n",
    "    \n",
    "    \n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    #build and train Model:\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    #set seed\n",
    "    ini=0\n",
    "    seed(ini+872527)\n",
    "    tf.random.set_seed(ini+87747)\n",
    "    \n",
    "    #build Model\n",
    "    inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2])) \n",
    "    lstm = tf.keras.layers.LSTM(32)(inp) \n",
    "    dense = tf.keras.layers.Dense(30, activation='relu')(lstm) \n",
    "    output = tf.keras.layers.Dense(1, activation='linear')(dense) \n",
    "    model = tf.keras.Model(inputs=inp, outputs=output)\n",
    "    #define optimizer\n",
    "    Adam = tf.keras.optimizers.Adam(learning_rate=1E-3, epsilon=1E-3, clipnorm=True)\n",
    "  \n",
    "    #compile the model\n",
    "    model.compile(loss='mse', optimizer=Adam, metrics=['mse'])\n",
    "\n",
    "    #define early stopping callback\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "    #train the model\n",
    "    history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                        batch_size=32, callbacks=[es])\n",
    "\n",
    "    #plot training loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.legend(['training loss','validation loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    #Test and evaluate\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    #Test model with X_opt\n",
    "    sim = model.predict(X_opt)\n",
    "\n",
    "    #rescale simulated and observed values\n",
    "    sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "    obs = scaler_gwl.inverse_transform(Y_opt.reshape(-1,1))\n",
    "    \n",
    "    #Mean Squared Error\n",
    "    MSE =  np.mean((sim-obs) ** 2)\n",
    "    \n",
    "    #Plot Optset Simulation to see something during Optimization\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(OptData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "    plt.plot(OptData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "    plt.title(\"Opt Result\",fontweight = 'bold')\n",
    "    plt.ylabel('GWL [m asl]')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend(fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.show()\n",
    "    \n",
    "    return -MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ec979",
   "metadata": {},
   "source": [
    "Nun haben wir eine Funktion geschrieben die uns in Abhängigkeit von n_steps_in ein Modell aufbaut, trainiert und den -MSE dafür ausgibt.\n",
    "\n",
    "\n",
    "Lasst uns nun die eigentliche Optimierung schreiben:\n",
    "### Bayesische Optimierung\n",
    "Zunächst laden wir das BayesianOptimization Package und die notwendigen Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674482eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import UtilityFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8607ad7",
   "metadata": {},
   "source": [
    "Nun legen wir in Form eines dictionaries fest welche HP optimiert werden sollen\n",
    "und in welchen Bereichen die Werte diese HP liegen sollen (parameter bounds).\n",
    "In unserem Fall enthält das Dictionary nur n_steps_in\n",
    "für mehrere HPs:\n",
    "```python\n",
    "pbounds = {'A': (AMin,AMax),\n",
    "           'B': (BMin,BMax)           } #etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2994aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {'n_steps_in': (1,50)} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96e97fa",
   "metadata": {},
   "source": [
    "Wir optimieren n_steps_in also von einer Woche bis 50 Wochen. Dies entspricht fast bis zu ein Jahr (52 Wochen), wir bleiben jedoch knapp darunter, weil wir sonst die Überlappung der Datensätze anders coden müssten, da unser Stopset nur 1 Jahr lang ist.\n",
    "\n",
    "Im folgenden führen wir nun fogende Schritte durch:\n",
    "1. Optimizer definieren\n",
    "2. Logger definieren\n",
    "3. Optimieren\n",
    "4. Nach Abschluss: optimalen Wert für *n_steps_in* auslesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define the actual optimizer\n",
    "optimizer = BayesianOptimization(\n",
    "    f= bayesoptfunction, #name of the function to be optimized (see above the function we wrote)\n",
    "    pbounds=pbounds, #value ranges in which to optimize\n",
    "    random_state=1, #seed\n",
    "    verbose = 0 # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent, verbose = 2 prints everything\n",
    "    )\n",
    "\n",
    "# Save progress: the logger writes the results of the iteration in a json file:\n",
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "\n",
    "optimizer.set_gp_params(normalize_y=True)\n",
    "\n",
    "# Create an instance of UtilityFunction for the acquisition function\n",
    "acquisition_function = UtilityFunction(kind=\"ei\", xi=0.05)\n",
    "\n",
    "# Start the optimization\n",
    "optimizer.maximize(\n",
    "    init_points=5,         # Steps of random exploration (random starting points)\n",
    "    n_iter=15,             # Steps of Bayesian optimization\n",
    "    acquisition_function=acquisition_function\n",
    ")\n",
    "\n",
    "#After completing the optimization, we want to read out the best value for n_steps_in (as integer: int(...)):    \n",
    "# -> get best values from optimizer\n",
    "\n",
    "n_steps_in= int(optimizer.max.get(\"params\").get(\"n_steps_in\")) # auch hier wollen wir wieder den integer haben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot best Iteration:\n",
    "print(\"\\nBEST:\\t{}\".format(optimizer.max))\n",
    "\n",
    "#Check after which iteration the maximum has already been reached:\n",
    "beststep = False\n",
    "step = -1\n",
    "while not beststep:\n",
    "    step = step + 1\n",
    "    beststep = optimizer.res[step] == optimizer.max\n",
    "print(\"\\nbest step {:.0f} of {:.0f} steps in total\".format(step+1, len(optimizer.res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff105a",
   "metadata": {},
   "source": [
    "Nun wollen wir das Modell mit dem scheinbar optimalen Wert für n_steps_in trainieren und testen. Hierfür brauchen wir fast den gleichen Code wie in der Funktion (bayesoptfunction) oben, nur wenden wir das Modell jetzt auf Testdaten statt Optdaten an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396fe827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have already defined n_steps_in\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#Now the familiar steps as in the last appointment for time series prediction.\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#extend with overlap to be able to fill sequence (n_steps_in) later                                             \n",
    "StopData_ext = pd.concat([TrainingData.iloc[-n_steps_in:], StopData], axis=0) #takes last steps of TrainingData and combines it with StopData\n",
    "# OptData_ext = pd.concat([StopData.iloc[-n_steps_in:], OptData], axis=0) #takes last steps of ValidationData and combines it with OptData\n",
    "TestData_ext = pd.concat([OptData.iloc[-n_steps_in:], TestData], axis=0) #takes last steps of OptData and combines it with TestData\n",
    "\n",
    "#create scalers and fit to data data (normalize: *_n):\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(TrainingData) # fit on training data, scale all other parts afterwards\n",
    "\n",
    "scaler_gwl = MinMaxScaler(feature_range=(-1, 1)) \n",
    "scaler_gwl.fit(pd.DataFrame(TrainingData['GWL'])) # fit scaler only on GWL for rescaling later\n",
    "\n",
    "#scale (transform) data\n",
    "TrainingData_n = scaler.transform(TrainingData)\n",
    "StopData_ext_n = scaler.transform(StopData_ext)\n",
    "TestData_ext_n = scaler.transform(TestData_ext)\n",
    "\n",
    "#Data to sequences:\n",
    "X_train,Y_train = make_sequences(np.asarray(TrainingData_n), n_steps_in)\n",
    "X_stop,Y_stop = make_sequences(np.asarray(StopData_ext_n), n_steps_in)\n",
    "X_test,Y_test = make_sequences(np.asarray(TestData_ext_n), n_steps_in)\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#Build and train Model:\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#set seed\n",
    "ini=0\n",
    "seed(ini+872527)\n",
    "tf.random.set_seed(ini+87747)\n",
    "\n",
    "#build model\n",
    "inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2])) \n",
    "lstm = tf.keras.layers.LSTM(32)(inp) \n",
    "dense = tf.keras.layers.Dense(30, activation='relu')(lstm) \n",
    "output = tf.keras.layers.Dense(1, activation='linear')(dense) \n",
    "model = tf.keras.Model(inputs=inp, outputs=output)\n",
    "\n",
    "\n",
    "#define optimizer\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=1E-3, epsilon=1E-3, clipnorm=True)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='mse', optimizer=Adam, metrics=['mse'])\n",
    "\n",
    "#define early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                    batch_size=32, callbacks=[es])\n",
    "\n",
    "#plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#Testing\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#test Model with X_test\n",
    "sim = model.predict(X_test)\n",
    "\n",
    "#rescale simulated and observed values\n",
    "sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "obs = scaler_gwl.inverse_transform(Y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ea611",
   "metadata": {},
   "source": [
    "Nun berechnen wir wieder Fehlerwerte und Plotten das Ergebnis (analog zu oben):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "#Nash-Sutcliffe-Efficiency: NSE\n",
    "NSE = 1 - ((np.sum((sim-obs) ** 2)) / (np.sum((obs - np.mean(obs)) ** 2)))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "RMSE =  np.sqrt(np.mean((sim-obs) ** 2))\n",
    "\n",
    "#Pearson r\n",
    "r = stats.pearsonr(sim[:,0], obs[:,0])\n",
    "r = r[0] #r\n",
    "\n",
    "#Bias\n",
    "Bias = np.mean(sim-obs)\n",
    "\n",
    "scores = pd.DataFrame(np.array([[NSE, RMSE, r, Bias]]),\n",
    "                      columns=['NSE','RMSE','r','Bias'])\n",
    "\n",
    "#Plot testset Simulation\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(TestData.index, sim, 'r', label =\"simulated\", linewidth = 1.7)\n",
    "plt.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "plt.title(\"Test Result\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,bbox_to_anchor=(1.12, 1),loc='upper right',fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print(\"n_steps_in = {:.0f} weeks\".format(n_steps_in))\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1a163",
   "metadata": {},
   "source": [
    "### Aufgabe 1: zusätzlichen Hyperparameter optimieren\n",
    "\n",
    "Wir möchten neben n_steps_in nun auch die Anzahl der LSTm units im LSTM Layer optimieren. Der Parameter soll *n* heißen.\n",
    "\n",
    "Ändere den obigen Code so, dass beide Parameter optimiert werden. nutze dafür folende Anleitung:  \n",
    "* Füge *n* der Funktion *bayesoptfunction* als Input parameter hinzu (gnau wie n_steps_in):\n",
    "```python\n",
    "    def bayesoptfunction(n_steps_in,n): \n",
    "```\n",
    "* Wandle *n* zu Beginn der Funktion in einen integer um (genau wie n_steps_in)\n",
    "* Füge *n* an entsprechener Stelle im LSTM Layer ein\n",
    "```python\n",
    "    ...(units=n)... \n",
    "```\n",
    "* Optimiere *n* für einen Wertebereich von 10 bis 50 (Parameter dem pbounds Dict hinzufügen)\n",
    "```python\n",
    "pbounds = {'A': (AMin,AMax),\n",
    "           'B': (BMin,BMax)           } #etc...\n",
    "```\n",
    "* Entnehme den besten Wert für n dem optimizer mit folgendem Befehl: \n",
    "```python\n",
    "n = int(optimizer.max.get(\"params\").get(\"n\"))* \n",
    "```\n",
    "* Übernehme den gefundenen Wert für *n* auch im Code für das Model, welches für die Testdaten verwendet wird\n",
    "\n",
    "Tipp: Entweder du änderst direkt den Code den du oberhalb findest (dann am besten das Notebook als Kopie abspeichern), oder kopiere dir den Code nach unten und erstellst neue Zellen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570beae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46ecbc38",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Einfaches Modell Ensemble\n",
    "\n",
    "Alle ANN werden mit (quasi)zufälligen Werten der Gewichte initialisiert. Um das Modell reproduzierbar zu halten, setzen wir immer den random seed vor jedem Durchlauf. Dies können wir aber auch nutzen um ein Ensemble zu erstellen, welches uns zeigt wie stark das Ergebnis vom random seed abhängt. ANstatt nun also immer den gleichen seed zu wählen, definieren wir mehrere Modelle mit unterschiedlichen Initialisierungen. \n",
    "\n",
    "Schreibe daher die übernächste Notebookzelle (Nur eine Kopie des Codes von oben) als **Schleife** bei der der Parameter *ini* von 0 bis 4 (also 5 verschiedene Werte: 0,1,2,3,4) varriert wird. Vergiss dafür nicht alle Zeilen nach dem Einrichten der Schleife einzurücken. \n",
    "\n",
    "Wir definieren zuvor die Variable *sim_all* in die die Ergebnisse aller Durchläufe jeweils als Spalte eingefügt werden sollen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f544445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_all = np.zeros((Y_test.shape[0],5)) #This is called preallocation, \n",
    "#further down in the code, after the loop has been inserted, at each pass the result should be stored as follows: \n",
    "#sim_all[:,ini] = sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2ee2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ini=0# The loop should start here (instead of ini=0)\n",
    "seed(ini+872527)\n",
    "tf.random.set_seed(ini+87747)\n",
    "\n",
    "#build model\n",
    "inp = tf.keras.Input(shape=(n_steps_in, X_train.shape[2])) \n",
    "lstm = tf.keras.layers.LSTM(32)(inp) \n",
    "dense = tf.keras.layers.Dense(30, activation='relu')(lstm) \n",
    "output = tf.keras.layers.Dense(1, activation='linear')(dense) \n",
    "model = tf.keras.Model(inputs=inp, outputs=output)\n",
    "\n",
    "#define optimizer\n",
    "Adam = tf.keras.optimizers.Adam(learning_rate=1E-3, epsilon=1E-3, clipnorm=True)\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='mse', optimizer=Adam, metrics=['mse'])\n",
    "\n",
    "#define early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights = True)\n",
    "\n",
    "#train the model\n",
    "history = model.fit(X_train, Y_train,validation_data=(X_stop, Y_stop), epochs=100, verbose=3,\n",
    "                    batch_size=32, callbacks=[es])\n",
    "\n",
    "#plot training loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training loss','validation loss'])\n",
    "plt.title('Loss')\n",
    "plt.show()\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#Testing \n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Modell testen mit X_test\n",
    "sim = model.predict(X_test)\n",
    "\n",
    "#rescale simulated and observed values\n",
    "sim = scaler_gwl.inverse_transform(sim)  # retransform to original scale\n",
    "obs = scaler_gwl.inverse_transform(Y_test.reshape(-1,1))\n",
    "\n",
    "#fill sim_all here column by column according to the note above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cc01f",
   "metadata": {},
   "source": [
    "Wenn ihr erfolgreich seid, könnt ihr die nächsten Zellen einfach laufen lassen ohne etwas zu ändern. Am Ende sollten ihr einen Plot herausbekommen, der euch alle Modelle sowie den Ensemblemittelwert zeigt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a577fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mean = np.mean(sim_all,axis = 1).reshape(-1,1) #wir berechnen die Fheler für den Mittelwert des Ensembles\n",
    "\n",
    "#Nash-Sutcliffe-Efficiency: NSE\n",
    "NSE = 1 - ((np.sum((sim_mean-obs) ** 2)) / (np.sum((obs - np.mean(obs)) ** 2)))\n",
    "\n",
    "#Root Mean Squared Error\n",
    "RMSE =  np.sqrt(np.mean((sim_mean-obs) ** 2))\n",
    "\n",
    "#Pearson r\n",
    "r = stats.pearsonr(sim_mean[:,0], obs[:,0])\n",
    "r = r[0] #r\n",
    "\n",
    "#Bias\n",
    "Bias = np.mean(sim_mean-obs)\n",
    "\n",
    "scores = pd.DataFrame(np.array([[NSE, RMSE, r, Bias]]),\n",
    "                      columns=['NSE','RMSE','r','Bias'])\n",
    "\n",
    "#Plot testset Simulation\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(TestData.index, sim_all[:,i], 'r', label = None, linewidth = 1, alpha = 0.5)\n",
    "\n",
    "plt.plot(TestData.index, sim_mean, 'r', label =\"simulated mean\", linewidth = 1.7)\n",
    "plt.plot(TestData.index, obs, 'k', label =\"observed\", linewidth=1.7,alpha=0.9)\n",
    "\n",
    "plt.title(\"Test Result\", size=17,fontweight = 'bold')\n",
    "plt.ylabel('GWL [m asl]', size=15)\n",
    "plt.xlabel('Date',size=15)\n",
    "plt.legend(fontsize=15,fancybox = False, framealpha = 1, edgecolor = 'k')\n",
    "plt.tight_layout()\n",
    "plt.grid(visible=True, which='major', color='#666666', alpha = 0.3, linestyle='-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe7e19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
