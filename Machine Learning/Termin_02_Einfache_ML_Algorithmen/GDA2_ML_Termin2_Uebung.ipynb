{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f771b45",
   "metadata": {},
   "source": [
    "# Termin 2 - Machine Learning Algorithmen (Aufgabe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7539998-4c11-4853-a763-468c4bee0142",
   "metadata": {},
   "source": [
    "## Einf√ºhrung in das Maschinelle Lernen  \n",
    "### √úberblick: √úberwachtes vs. Un√ºberwachtes Lernen\n",
    "\n",
    "Maschinelles Lernen ist ein Teilbereich der k√ºnstlichen Intelligenz. Dabei lernt ein Modell aus vorhandenen Daten, um Vorhersagen oder Entscheidungen zu treffen ‚Äì ohne explizit programmiert zu werden.\n",
    "\n",
    "---\n",
    "\n",
    "### √úberwachtes Lernen (Supervised Learning)\n",
    "\n",
    "Beim √ºberwachten Lernen wird ein Modell mit **gelabelten Trainingsdaten** trainiert. Das bedeutet: Die Daten enthalten sowohl Eingabewerte als auch Zielwerte (Labels), anhand derer das Modell lernt.\n",
    "\n",
    "#### Typische Aufgaben:\n",
    "\n",
    "- **Klassifikation**  \n",
    "  Vorhersage einer Kategorie, z.‚ÄØB.:\n",
    "  - Bin√§r: Ja/Nein, Krank/Gesund\n",
    "  - Mehrklassen: Hund, Katze, Vogel\n",
    "\n",
    "- **Regression**  \n",
    "  Vorhersage eines kontinuierlichen Werts, z.‚ÄØB.:  \n",
    "  Grundwasserstand in Abh√§ngigkeit von Niederschlag und Landnutzung\n",
    "\n",
    "#### H√§ufig verwendete Algorithmen:\n",
    "\n",
    "- Logistische Regression  \n",
    "- Entscheidungsb√§ume (Decision Trees)  \n",
    "- Random Forest  \n",
    "- Support Vector Machines (SVM)\n",
    "\n",
    "---\n",
    "\n",
    "### Un√ºberwachtes Lernen (Unsupervised Learning)\n",
    "\n",
    "Hier gibt es **keine Zielwerte**. Das Modell versucht eigenst√§ndig, Muster oder Strukturen in den Daten zu erkennen.\n",
    "\n",
    "#### Typische Methoden:\n",
    "\n",
    "- K-Means-Clustering: Gruppiert √§hnliche Datenpunkte  \n",
    "- Principal Component Analysis (PCA): Reduziert die Anzahl der Merkmale (Dimensionsreduktion)\n",
    "\n",
    "---\n",
    "\n",
    "Weitere Informationen zu den in `scikit-learn` verf√ºgbaren Algorithmen f√ºr √ºberwachtes Lernen findet ihr unter:  \n",
    "https://scikit-learn.org/stable/supervised_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc56e4",
   "metadata": {},
   "source": [
    "# Teil 1 ‚Äì √úberwachtes Lernen\n",
    "\n",
    "### Bin√§re Klassifikation\n",
    "\n",
    "Bei der bin√§ren Klassifikation wird ein Modell darauf trainiert, zwischen genau **zwei Klassen** zu unterscheiden.  \n",
    "Beispiel: Vorhersage, ob ein Patient krank oder gesund ist (Ja/Nein).\n",
    "\n",
    "---\n",
    "\n",
    "### Datensatz: Einlesen und Vorbereitung\n",
    "\n",
    "- **Datenbasis**: Grundwasser-Stichtagsmessungen aus dem Jahr 2005  \n",
    "- **Zielvariable**: Landnutzung ‚Äì Vorhersage, ob landwirtschaftlich genutzt (Ja/Nein)  \n",
    "- **Problemtyp**: Bin√§re Klassifikation basierend auf Umweltmessdaten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f3cc5",
   "metadata": {},
   "source": [
    "### Vorbereitung: Wichtige Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grundlagen ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Datenvorverarbeitung ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Klassifikationsmodelle ---\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- Bewertung des Modells ---\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Pipeline zur Modellierung ---\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einladen der Daten\n",
    "filename = 'gwdata_2005.csv'\n",
    "dataset = pd.read_csv(filename, delimiter=';', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Preprocessing (Bereinigen von NaN-Werten und Entfernen von nicht ben√∂tigten Spalten)\n",
    "dataset_cleaned = dataset.dropna(axis=0, how='any')\n",
    "data = dataset_cleaned.drop(['GWNum','Messstelle','Rechtswert', 'Hochwert', 'Aquifer','Aquifer2','landuse'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c06947-30ad-447e-bcf8-cf35b8885860",
   "metadata": {},
   "source": [
    "### Explorative √úbersicht √ºber den Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217de704-59bb-442c-aa35-d7798b19a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úberblick √ºber Struktur, Inhalte und Korrelationen\n",
    "print(\"Form des Datensatzes:\", data.shape)\n",
    "print(\"\\nDatentypen:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "print(\"\\nStatistische Kennzahlen:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nEindeutige Werte pro Spalte:\")\n",
    "print(data.nunique())\n",
    "\n",
    "print(\"\\nFehlende Werte pro Spalte:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nKorrelationen (numerisch):\")\n",
    "print(data.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a3d8d-16be-4d15-a003-100430b3b051",
   "metadata": {},
   "source": [
    "### Zielvariable: `landuse_num`\n",
    "\n",
    "Die Spalte `landuse_num` ist die Zielvariable f√ºr die Klassifikation. Sie kodiert landwirtschaftliche Nutzung bin√§r:\n",
    "\n",
    "- `0`: keine Landwirtschaft (`no_agr`)\n",
    "- `1`: landwirtschaftlich genutzt (`agr`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d08472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl der Klassen in 'landuse_num':\")\n",
    "print(data['landuse_num'].value_counts().rename(index={0: 'Keine Landwirtschaft', 1: 'Landwirtschaft'}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67215fc",
   "metadata": {},
   "source": [
    "### Aufteilen in Trainings- und Testdaten\n",
    "\n",
    "Um die Modellleistung objektiv zu bewerten, wird der Datensatz in zwei Teile getrennt:\n",
    "\n",
    "- **Trainingsdaten**: zum Trainieren des Modells\n",
    "- **Testdaten**: zur √úberpr√ºfung der Generalisierungsf√§higkeit\n",
    "\n",
    "Typisches Verh√§ltnis: 80‚ÄØ% Training, 20‚ÄØ% Test (Standardwert).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zufalls-Seed f√ºr Reproduzierbarkeit\n",
    "random_state = 42\n",
    "\n",
    "# Eingabedaten (X) und Zielvariable (y)\n",
    "X = data.drop('landuse_num', axis=1)\n",
    "y = data['landuse_num']\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    ")\n",
    "\n",
    "# Formen der Datens√§tze anzeigen\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9da9-d551-4799-b179-5f91c5ce1962",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines sind leistungsf√§hige Verfahren f√ºr Klassifikation und Regression.  \n",
    "Sie arbeiten, indem sie eine **Entscheidungsgrenze** (Hyperebene) im Merkmalsraum bestimmen, die die Klassen bestm√∂glich voneinander trennt.\n",
    "\n",
    "#### Support Vector Classification (SVC)\n",
    "\n",
    "`SVC` ist die SVM-Variante f√ºr Klassifikationsaufgaben. Sie eignet sich besonders f√ºr komplexe, auch nicht-linear trennbare Probleme ‚Äì etwa durch den Einsatz von Kernfunktionen (Kernels).\n",
    "\n",
    "---\n",
    "\n",
    "Weitere Informationen in der scikit-learn-Dokumentation:\n",
    "\n",
    "- [SVM in scikit-learn](https://scikit-learn.org/stable/modules/svm.html)  \n",
    "- [Modellbewertung & Scoring](https://scikit-learn.org/stable/modules/model_evaluation.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduzierbarkeit sicherstellen\n",
    "random_state = 88\n",
    "\n",
    "# Bewertungsmetrik f√ºr Cross-Validation\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Anzahl der Folds f√ºr K-Fold-Validierung\n",
    "n_splits = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b544c",
   "metadata": {},
   "source": [
    "### 1/3: SVC mit unskalierten Eingabedaten\n",
    "\n",
    "In diesem Schritt wird ein `SVC`-Modell (Support Vector Classification) mit den unskalierten Eingabedaten trainiert.\n",
    "\n",
    "Support Vector Machines sind empfindlich gegen√ºber ungleich skalierten oder unterschiedlich verteilten Merkmalen.  \n",
    "Fehlende Standardisierung kann die Modellg√ºte deutlich beeintr√§chtigen. Dieses Modell dient daher als Referenz f√ºr sp√§tere Vergleiche mit skalierten Varianten.\n",
    "\n",
    "[Mehr zur SVC in der scikit-learn-Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM-Modell mit RBF-Kernel (ohne Skalierung)\n",
    "svm_raw = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# K-Fold-Setup\n",
    "kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung\n",
    "cv_scores_svm_raw = cross_val_score(svm_raw, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Mittelwert und Standardabweichung der Genauigkeit\n",
    "mean_accuracy_raw = cv_scores_svm_raw.mean()\n",
    "std_accuracy_raw = cv_scores_svm_raw.std()\n",
    "\n",
    "# Ergebnis ausgeben\n",
    "print(f\"SVM Accuracy (ohne Skalierung): {mean_accuracy_raw:.3f} ¬± {std_accuracy_raw:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d9083",
   "metadata": {},
   "source": [
    "### 2/3: SVC mit skalierten Eingabedaten\n",
    "\n",
    "Durch Standardisierung (z.‚ÄØB. mit `StandardScaler`) werden alle Merkmale auf denselben Wertebereich gebracht.  \n",
    "Dies kann die Leistung von SVMs erheblich verbessern.\n",
    "\n",
    "Im Folgenden wird dasselbe `SVC`-Modell wie zuvor verwendet, diesmal jedoch mit vorgeschalteter Skalierung der Eingabedaten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d16910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen und Anwenden des StandardScalers auf die Trainingsdaten\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# SVM-Modell mit RBF-Kernel (auf skalierten Daten)\n",
    "svm_scaled = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# K-Fold-Cross-Validation definieren\n",
    "kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung mit skalierten Daten\n",
    "cv_scores_svm_scaled = cross_val_score(svm_scaled, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Genauigkeit und Streuung berechnen\n",
    "mean_accuracy_scaled = cv_scores_svm_scaled.mean()\n",
    "std_accuracy_scaled = cv_scores_svm_scaled.std()\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"SVM Accuracy (mit Skalierung): {mean_accuracy_scaled:.3f} ¬± {std_accuracy_scaled:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ed19c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f0ff; padding:10px; border-radius:5px; font-weight:bold\">\n",
    "   For Your Interest: Pipeline mit mehreren Verarbeitungsschritten\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Der folgende Abschnitt zeigt, wie man mithilfe von `Pipeline` mehrere Verarbeitungsschritte nahtlos kombiniert ‚Äì von der Vorverarbeitung bis zur Modellierung mit einem SVM.\n",
    "\n",
    "**Typische Komponenten einer solchen Pipeline:**\n",
    "\n",
    "- **Imputation**:  \n",
    "  `SimpleImputer(strategy='mean')`  \n",
    "  ‚Üí Ersetzt fehlende Werte durch den Mittelwert der jeweiligen Spalte.\n",
    "\n",
    "- **Skalierung**:  \n",
    "  `StandardScaler()`  \n",
    "  ‚Üí Skaliert alle Merkmale auf dieselbe Gr√∂√üenordnung.\n",
    "\n",
    "- **Feature Engineering (optional)**:  \n",
    "  `PolynomialFeatures(degree=2)`  \n",
    "  ‚Üí F√ºgt polynomiale Merkmale hinzu, um nichtlineare Zusammenh√§nge abzubilden.\n",
    "\n",
    "- **Feature Selection**:  \n",
    "  `SelectKBest(f_classif, k=10)`  \n",
    "  ‚Üí W√§hlt die 10 Merkmale mit der h√∂chsten Relevanz anhand des ANOVA-F-Tests aus.\n",
    "\n",
    "- **Dimensionsreduktion**:  \n",
    "  `PCA(n_components=0.95)`  \n",
    "  ‚Üí Reduziert die Dimension, wobei 95‚ÄØ% der Varianz erhalten bleiben.\n",
    "\n",
    "- **Klassifikator**:  \n",
    "  `SVC(kernel='rbf', gamma='auto')`  \n",
    "  ‚Üí SVM mit RBF-Kernel f√ºr die Klassifikation.\n",
    "\n",
    "---\n",
    "\n",
    "> **Hinweis:**  \n",
    "> Der folgende Code dient der Veranschaulichung.  \n",
    "> Einige der verwendeten Komponenten sind m√∂glicherweise **nicht in der aktuellen Umgebung installiert** und daher **nicht direkt ausf√ºhrbar**.\n",
    "```\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('feature_select', SelectKBest(f_classif, k=10)),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('svm', SVC(kernel='rbf', gamma='auto'))\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be77ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: Skalierung + SVM-Modell\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', gamma='auto'))\n",
    "])\n",
    "\n",
    "# K-Fold-Cross-Validation definieren\n",
    "kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung mit der Pipeline\n",
    "cv_scores_svm_pipeline = cross_val_score(svm_pipeline, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Genauigkeit berechnen\n",
    "mean_accuracy_pipeline = cv_scores_svm_pipeline.mean()\n",
    "std_accuracy_pipeline = cv_scores_svm_pipeline.std()\n",
    "\n",
    "# Ausgabe der mittleren Genauigkeit ¬± Standardabweichung\n",
    "print(f\"SVM Accuracy (Pipeline): {mean_accuracy_pipeline:.3f} ¬± {std_accuracy_pipeline:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41499c27",
   "metadata": {},
   "source": [
    "### 3/3: Random Forest Classifier\n",
    "\n",
    "Der `RandomForestClassifier` ist ein Ensemble-Lernverfahren, das auf einer Vielzahl von Entscheidungsb√§umen basiert.  \n",
    "Durch das sogenannte Bagging (Bootstrap Aggregation) und die zuf√§llige Auswahl von Features bei jedem Split werden √úberanpassung reduziert und die Generalisierungsf√§higkeit erh√∂ht.\n",
    "\n",
    "Vorteile:\n",
    "- Robust gegen√ºber Ausrei√üern und Rauschen\n",
    "- Kaum anf√§llig f√ºr Overfitting bei ausreichender Baumanzahl\n",
    "- Gut skalierbar und vielseitig einsetzbar (Klassifikation und Regression)\n",
    "\n",
    "[Weitere Informationen zum `RandomForestClassifier` in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest-Modell mit Standardparametern\n",
    "rf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# K-Fold-Cross-Validation definieren\n",
    "kfold = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung mit skalierten Eingabedaten\n",
    "cv_scores_rf = cross_val_score(rf, X_train_scaled, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Genauigkeit und Standardabweichung berechnen\n",
    "mean_accuracy_rf = cv_scores_rf.mean()\n",
    "std_accuracy_rf = cv_scores_rf.std()\n",
    "\n",
    "# Ausgabe\n",
    "print(f\"Random Forest Accuracy: {mean_accuracy_rf:.3f} ¬± {std_accuracy_rf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30cc7e",
   "metadata": {},
   "source": [
    "### Vergleich der Ergebnisse\n",
    "\n",
    "Im Folgenden werden die mittleren Genauigkeiten der drei Modelle gegen√ºbergestellt:\n",
    "\n",
    "- **SVC ohne Skalierung**\n",
    "- **SVC mit Skalierung**\n",
    "- **Random Forest Classifier**\n",
    "\n",
    "Der Vergleich dient zur Veranschaulichung der Auswirkungen von Vorverarbeitung und Modellwahl auf die Klassifikationsgenauigkeit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e662b-fd56-41b1-8d72-b4a46e746fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse aus Cross-Validation (Accuracy-Werte pro Fold)\n",
    "results = [\n",
    "    cv_scores_svm_raw,        # SVM Unskaliert\n",
    "    cv_scores_svm_scaled,      # SVM mit manueller Skalierung   \n",
    "    cv_scores_rf,             # Random Forest\n",
    "]\n",
    "\n",
    "# Boxplot zum Vergleich der Modelle\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(results, patch_artist=True)\n",
    "plt.title('Vergleich der Modelle (Kreuzvalidierung)')\n",
    "plt.xticks([1, 2, 3], ['SVM', 'SVM (scaled)', 'Random Forest'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e58bd1-4d7f-4823-8966-90991f16e0a9",
   "metadata": {},
   "source": [
    "### Auswertung des besten Modells\n",
    "\n",
    "Nach dem Vergleich der Modelle anhand ihrer Kreuzvalidierungsergebnisse wird in diesem Abschnitt das Modell mit der h√∂chsten mittleren Genauigkeit n√§her ausgewertet.\n",
    "\n",
    "Die folgenden Analyseschritte k√∂nnen dabei durchgef√ºhrt werden:\n",
    "\n",
    "- Training des Modells auf dem vollst√§ndigen Trainingsdatensatz\n",
    "- Vorhersage auf dem Testdatensatz (`X_test`)\n",
    "- Berechnung relevanter Metriken:\n",
    "  - Konfusionsmatrix\n",
    "  - Genauigkeit, Pr√§zision, Recall, F1-Score\n",
    "- Optional: Visualisierung der Konfusionsmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe49df5-75ad-48da-a760-917ec418eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell f√ºr finale Auswertung auf dem Testdatensatz\n",
    "rf_final = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Skalierung der Testdaten mit dem bereits auf Trainingsdaten gefitteten Scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training des Random Forest auf dem gesamten (skalierten) Trainingsdatensatz\n",
    "rf_final.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Vorhersage auf dem (skalierten) Testdatensatz\n",
    "y_pred = rf_final.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genauigkeit auf dem Testdatensatz berechnen\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Konfusionsmatrix berechnen\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Klassifikationsbericht mit Precision, Recall, F1-Score\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609941d-52ec-4ca8-a359-d94a5538ad10",
   "metadata": {},
   "source": [
    "**Ergebnis:**  \n",
    "Das Modell erkennt Klasse 0 (*keine Landwirtschaft*) sehr gut (**Recall: 89‚ÄØ%**).  \n",
    "Bei Klasse 1 (*Landwirtschaft*) gibt es gr√∂√üere Probleme:  \n",
    "Die **Precision** (Wie zuverl√§ssig ist das Modell, wenn es ‚Äû1‚Äú sagt) ist gut,  \n",
    "der **Recall** (Wie viele echte ‚Äû1‚Äú wurden erkannt) jedoch nur mittelm√§√üig.  \n",
    "Der **F1-Score** (Kompromiss zwischen beiden) liegt entsprechend im mittleren Bereich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a887-df3b-4d94-9405-aa771a4aa4ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Multiklassen-Klassifikation\n",
    "Viele reale Klassifikationsprobleme bestehen nicht nur aus zwei, sondern aus mehreren Klassen.  \n",
    "Multiklassen-Klassifikation bezeichnet die Vorhersage einer von mehr als zwei m√∂glichen Zielklassen.\n",
    "#### IRIS-Datensatz laden\n",
    "\n",
    "Der Iris-Datensatz enth√§lt 150 Beispiele von Blumen, die jeweils einer von drei Arten angeh√∂ren (*Setosa*, *Versicolor* oder *Virginica*).  \n",
    "Er besteht aus vier numerischen Merkmalen: L√§nge und Breite von Kelchblatt (*Sepal*) und Kronblatt (*Petal*).\n",
    "\n",
    "Dieser Datensatz eignet sich gut, um verschiedene Machine-Learning-Modelle hinsichtlich ihrer F√§higkeit zur Klassifikation mehrerer Klassen zu testen.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" width=\"75%\">\n",
    "</div>\n",
    "\n",
    "Der Datensatz wird als sogenanntes `Bunch`-Objekt geladen. Dieses enth√§lt neben den Eingabedaten auch Zusatzinformationen wie z.‚ÄØB.:\n",
    "\n",
    "- **data**: Ein NumPy-Array mit den Merkmalswerten.\n",
    "- **target**: Ein NumPy-Array mit den Zielwerten (0, 1, 2).\n",
    "- **feature_names**: Namen der Merkmale (z.‚ÄØB. `sepal length`).\n",
    "- **target_names**: Namen der Zielklassen (`setosa`, `versicolor`, `virginica`).\n",
    "- **DESCR**: Beschreibung des Datensatzes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a184fd8-b460-46e7-a061-a783fa79fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Laden des Iris-Datensatzes\n",
    "iris = load_iris()\n",
    "\n",
    "# Extrahieren der Merkmale und des Targets aus dem Iris-Datensatz\n",
    "X = iris.data  # Merkmale\n",
    "y = iris.target  # Zielwerte\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1667fee",
   "metadata": {},
   "source": [
    "## Aufgabe: Multiklassen-Klassifikation mit dem Iris-Datensatz\n",
    "\n",
    "F√ºhre eine Multiklassen-Klassifikation mit dem Iris-Datensatz durch.  \n",
    "Implementiere mindestens zwei Modelle:\n",
    "\n",
    "- Ein Modell auf Basis von **Support Vector Machines (SVM)**  \n",
    "  (einmal **ohne Skalierung** und einmal **mit Skalierung**)\n",
    "- Ein Modell auf Basis von **Random Forest (RF)**\n",
    "\n",
    "### Vorgehensweise:\n",
    "- Teile die Daten in Trainings- und Testdatens√§tze auf\n",
    "- Identifiziere relevante Merkmale f√ºr die Klassifikation\n",
    "- Implementiere die beiden Modelle\n",
    "- F√ºhre eine Evaluierung mit den Testdaten durch\n",
    "- Berechne die Genauigkeit der Vorhersagen\n",
    "- Vergleiche die Ergebnisse beider Modelle\n",
    "- Diskutiere, welches Modell besser geeignet ist\n",
    "\n",
    "---\n",
    "\n",
    "### Bearbeitungs√ºbersicht\n",
    "\n",
    "- [x] Laden des Iris-Datensatzes  \n",
    "- [ ] Aufteilen der Daten in Trainings- und Testdatens√§tze  \n",
    "- [ ] Identifikation relevanter Merkmale  \n",
    "- [ ] Implementierung eines SVM-Modells (unskaliert & skaliert)  \n",
    "- [ ] Implementierung eines Random Forest-Modells  \n",
    "- [ ] Evaluierung und Berechnung der Genauigkeit  \n",
    "- [ ] Vergleich und Diskussion der Ergebnisse  \n",
    "\n",
    "---\n",
    "\n",
    "### Hinweis:\n",
    "Achte auf eine angemessene Datenvorverarbeitung, insbesondere die **Skalierung der Eingabedaten** f√ºr SVM.  \n",
    "Teste au√üerdem unterschiedliche Hyperparameter, um die Modellleistung zu optimieren.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2a7ee-e5e3-4be9-bf57-dbe428eeaad8",
   "metadata": {},
   "source": [
    "### 1. Daten aufteilen\n",
    "\n",
    "- Erstelle X_train, X_test, Y_train, Y_test.\n",
    "- Definiere tesize und den random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2755a-d68b-41bd-bdc2-39fd16bd7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aufteilen in Trainings- und Testdaten (80/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Trainingsdaten:\", X_train.shape)\n",
    "print(\"Testdaten:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11c1e8-b67c-4a67-8396-4ccb00bf6dfa",
   "metadata": {},
   "source": [
    "### 2. SVM-Modell (ohne Skalierung)\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f8c3a3-bfea-42bf-88ad-c002ff6de9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM ohne Skalierung\n",
    "svm_raw = SVC(kernel='rbf', gamma='scale')\n",
    "svm_raw.fit(X_train, y_train)\n",
    "y_pred_svm_raw = svm_raw.predict(X_test)\n",
    "\n",
    "# Auswertung\n",
    "acc_svm_raw = accuracy_score(y_test, y_pred_svm_raw)\n",
    "print(f\"SVM Accuracy (ohne Skalierung): {acc_svm_raw:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd464b4b-1b2e-4310-87d6-c583b98e9491",
   "metadata": {},
   "source": [
    "### 3. SVM-Modell (mit Skalierung)\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0257751-7e63-4648-8f42-fec6d32a718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalierung\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM mit Skalierung\n",
    "svm_scaled = SVC(kernel='rbf', gamma='scale')\n",
    "svm_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_svm_scaled = svm_scaled.predict(X_test_scaled)\n",
    "\n",
    "# Auswertung\n",
    "acc_svm_scaled = accuracy_score(y_test, y_pred_svm_scaled)\n",
    "print(f\"SVM Accuracy (mit Skalierung): {acc_svm_scaled:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caaac4-58be-4ac7-b401-64da04d860c7",
   "metadata": {},
   "source": [
    "### 4. Random Forest-Modell\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e836c-d9bd-4cf7-9366-47fbcc652d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest (braucht keine Skalierung)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Auswertung\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {acc_rf:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51b2fbd-4530-4f36-9144-98993a653e7f",
   "metadata": {},
   "source": [
    "### 5. Vergleich der Modellgenauigkeiten\n",
    "\n",
    "- F√ºhre Ergebnisse zusammen und Plotte sie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb633-e063-4711-a2e0-2eec20fc1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Modellvergleich:\")\n",
    "print(f\"- SVM (ohne Skalierung):     {acc_svm_raw:.3f}\")\n",
    "print(f\"- SVM (mit Skalierung):      {acc_svm_scaled:.3f}\")\n",
    "print(f\"- Random Forest:             {acc_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21a16c-4d01-4c7d-8d14-34d647b83cd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Regression\n",
    "\n",
    "### Concrete-Datensatz\n",
    "\n",
    "Der Concrete-Datensatz enth√§lt verschiedene Eigenschaften von Betonmischungen und deren resultierende Druckfestigkeit.  \n",
    "Er umfasst **1.030 Datens√§tze** mit **8 Eingabevariablen** und einer Zielgr√∂√üe (Regression):\n",
    "\n",
    "**Eingabevariablen:**\n",
    "- Cement (*Zement*)\n",
    "- Blast Furnace Slag (*H√ºttenzement*)\n",
    "- Fly Ash (*Flugasche*)\n",
    "- Water (*Wasser*)\n",
    "- Superplasticizer (*Flie√ümittel*)\n",
    "- Coarse Aggregate (*Grobgestein*)\n",
    "- Fine Aggregate (*Feingestein*)\n",
    "- Age (*Alter in Tagen*)\n",
    "\n",
    "**Zielvariable:**\n",
    "- Concrete Compressive Strength (*Druckfestigkeit des Betons*, in MPa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353cc4-f9e9-4d45-afee-558f6117c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete-Datensatz laden\n",
    "data = pd.read_csv('Concrete_Data.csv', sep=';', encoding='ISO-8859-1')\n",
    "\n",
    "# Vorschau auf die ersten 5 Zeilen\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf38c6-c3b2-4190-91d0-100cff3fdf43",
   "metadata": {},
   "source": [
    "### Hinweis zur Modellbewertung (Regression)\n",
    "\n",
    "Da wir nun eine **Regression** durchf√ºhren (nicht Klassifikation), verwenden wir eine andere Metrik zur Bewertung der Modellg√ºte: den **R¬≤-Score**.\n",
    "\n",
    "Der R¬≤-Score (Bestimmtheitsma√ü) gibt an, wie gut die vorhergesagten Werte zu den tats√§chlichen Werten passen.  \n",
    "Ein Wert von 1 bedeutet perfekte Vorhersage, 0 entspricht dem Mittelwertsmodell, negative Werte deuten auf schlechte Modellanpassung hin.\n",
    "\n",
    "Wir verwenden den `r2_score` aus `sklearn.metrics`.\n",
    "\n",
    "Weitere Informationen und alternative Regressionsmetriken wie MAE oder RMSE findest du unter:  \n",
    "[scikit-learn.org ‚Äì Model Evaluation (Regression)](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62be961-2efd-4051-a367-4910a7433432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Einstellungen f√ºr Modellvergleich und Reproduzierbarkeit\n",
    "\n",
    "# Zufalls-Seed f√ºr Reproduzierbarkeit der Ergebnisse\n",
    "random_seed = 88\n",
    "\n",
    "# Bewertungsmetrik f√ºr Regressionsmodelle\n",
    "scoring_metric = 'r2'  # R¬≤ (Bestimmtheitsma√ü)\n",
    "\n",
    "# Anzahl der Folds f√ºr K-Fold Cross-Validation\n",
    "num_folds = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19b685-4738-41f5-bf2a-46c9aa0e60c3",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84f5a4-ad33-4dbd-b32b-ec17bb147e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setzen des Seeds f√ºr die Reproduzierbarkeit\n",
    "random_state = 42\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "X = data.drop('Druckfestigkeit', axis=1)  # Spalte Zielvariable ab\n",
    "Y = data['Druckfestigkeit']  # Definiere 'landuse_num' als Zielvariable\n",
    "test_size = 0.2  # Anteil der Testdaten: 20%\n",
    "\n",
    "# Mischen und Aufteilen der Daten in Trainings- und Testdaten\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "print(\"Form der Trainingsdaten (X_train):\", X_train.shape)\n",
    "print(\"Form der Testdaten (X_test):\", X_test.shape)\n",
    "print(\"Form der Trainingszielvariablen (Y_train):\", Y_train.shape)\n",
    "print(\"Form der Testzielvariablen (Y_test):\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa35317-ea7b-4291-b82b-2e9a2b7f800b",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 24px\"><strong>Aufgabe:</strong></span>  \n",
    "Entwickle zwei verschiedene Modelle zur Vorhersage der **Druckfestigkeit** des Betons:  \n",
    "‚Äì eines basierend auf **Support Vector Machines (SVM)**  \n",
    "‚Äì eines basierend auf **Random Forest (RF)**\n",
    "\n",
    "### Schritte:\n",
    "- [x] **Daten aufteilen:** Teile die Daten in Trainings- und Testdatens√§tze auf  \n",
    "- [ ] **Modelle implementieren:** Implementiere ein SVM-Modell und ein Random Forest-Modell  \n",
    "- [ ] **Trainieren & evaluieren:** Trainiere die Modelle und evaluiere sie auf den Testdaten  \n",
    "- [ ] **Genauigkeit berechnen:** Ermittle die Vorhersageg√ºte (z.‚ÄØB. R¬≤ oder MAE)  \n",
    "- [ ] **Ergebnisse vergleichen:** Vergleiche beide Modelle und beurteile ihre Eignung\n",
    "\n",
    "---\n",
    "\n",
    "### N√ºtzliche Links:\n",
    "\n",
    "- [Scikit-learn √úbersicht](https://scikit-learn.org)  \n",
    "- [SVR ‚Äì Support Vector Regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)  \n",
    "- [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)  \n",
    "- [Model Evaluation in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5fcbc-cc97-44ca-a8b3-364b6b58ce10",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 24px\">Support Vector Regressor</span>**\n",
    "\n",
    "**<span style=\"font-size: 18px\">Support Vector Regressor (ohne skalierung)</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700920db-401e-478f-b9ba-94201cafaff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353394fc-22cf-401a-811f-e73415107448",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px\">Support Vector Regressor skaliert</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad8649-17b9-467e-9b03-da8c5bc7db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39b0de3-c720-42a1-85cd-c05c3c88f887",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px\">Random Forest</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b7a27-15d9-448a-adf3-9d972cb13b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a89bb43-f196-4040-9053-b213f3361b5c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**Vergleich der Ergebnisse:**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc1420-18ef-4416-b1be-c1858aa77adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2fcbabd-65fc-478f-a07b-c7e604df75b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Un√ºberwachtes Lernen\n",
    "\n",
    "Die **PCA** (Principal Component Analysis) ist ein Verfahren zur **Dimensionsreduktion**. Sie reduziert die Anzahl der Variablen, w√§hrend die **wichtigste Varianz im Datensatz erhalten bleibt**.  \n",
    "Beim Iris-Datensatz kann PCA verwendet werden, um die vier Merkmale (Sepal-L√§nge/-Breite, Petal-L√§nge/-Breite) in zwei Hauptkomponenten zu √ºberf√ºhren. Das erleichtert die Visualisierung und zeigt Muster im Datenraum auf.\n",
    "\n",
    "**KMeans** ist ein klassischer **Clustering-Algorithmus**. Er teilt Datenpunkte in **k Cluster**, wobei jedes Cluster durch seinen **Zentroid (Mittelpunkt)** beschrieben wird.  \n",
    "Im Iris-Datensatz k√∂nnen mit KMeans Cluster gebildet werden, die evtl. den drei Iris-Arten √§hneln ‚Äì ohne dass die Labels bekannt sein m√ºssen.\n",
    "\n",
    "---\n",
    "\n",
    "### KMeans Clustering\n",
    "\n",
    "#### Datensatz laden\n",
    "\n",
    "Wir verwenden erneut den **Iris-Datensatz**, verzichten nun aber auf die Zielvariablen (`target`), da es sich um **un√ºberwachtes Lernen** handelt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Iris-Daten ohne Zielvariable laden\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# In DataFrame umwandeln und anzeigen\n",
    "df_iris = pd.DataFrame(X, columns=feature_names)\n",
    "print(df_iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de39f14-3e4b-4a5b-8e50-be3eb07b53e1",
   "metadata": {},
   "source": [
    "### Merkmalskombinationen visuell vergleichen\n",
    "\n",
    "Der Iris-Datensatz besteht aus vier numerischen Merkmalen:  \n",
    "Bei vier Merkmalen ergeben sich insgesamt **6 m√∂gliche 2D-Kombinationen**, in denen jeweils zwei Merkmale gegen√ºbergestellt werden k√∂nnen.  \n",
    "\n",
    "Diese Visualisierung zeigt alle m√∂glichen **Paarungen der Merkmale als Streudiagramme**.  \n",
    "Einzelne Gruppen oder Muster lassen sich teilweise erkennen, jedoch ist eine **klare visuelle Trennung oder Gruppierung rein durch das Auge schwierig**, insbesondere ohne Farbcodierung nach Klassen oder Cluster.\n",
    "\n",
    "Diese Streudiagramme bieten dennoch eine erste Einsch√§tzung m√∂glicher Zusammenh√§nge oder Trennbarkeit in bestimmten Merkmalskombinationen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec318993-86bd-4151-8bf0-d81ef5de487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alle Kombinationen von 2 Features\n",
    "pairs = [(0, 1), (0, 2), (0, 3),\n",
    "         (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "# 2x3 Subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, (i, j) in enumerate(pairs):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    ax.scatter(X[:, i], X[:, j], s=50, alpha=0.7)\n",
    "    ax.set_xlabel(feature_names[i])\n",
    "    ax.set_ylabel(feature_names[j])\n",
    "    ax.set_title(f'{feature_names[i]} vs. {feature_names[j]}')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711f972-da89-414b-9edc-c01e0cad88b6",
   "metadata": {},
   "source": [
    "Da der Iris-Datensatz drei Klassen umfasst (**Setosa**, **Versicolor**, **Virginica**), ist es sinnvoll, die Daten mithilfe der **PCA** auf **drei Dimensionen** zu reduzieren.\n",
    "\n",
    "So l√§sst sich ein Gro√üteil der Varianz im Datensatz bewahren und gleichzeitig eine kompakte Repr√§sentation erzeugen, die sich gut f√ºr **Visualisierung** und **Clusteranalyse** eignet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# KMeans-Clustering mit 3 Clustern\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Cluster-Zuweisung f√ºr jeden Datenpunkt\n",
    "cluster_labels = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e04de0-e760-4f4a-8fa0-ecca31aa4b11",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Validierung</span>**\n",
    "\n",
    "F√ºr das KMeans-Clustering gibt es verschiedene Metriken zur Bewertung der Clusterqualit√§t, darunter:\n",
    "\n",
    "- **Inertia**\n",
    "- **Silhouette Score**\n",
    "- **Calinski-Harabasz Index**\n",
    "- **Davies-Bouldin Index**\n",
    "\n",
    "Diese Metriken geben Einblicke in die Trennsch√§rfe, Kompaktheit und Struktur der erkannten Cluster.  \n",
    "Details zu deren Anwendung in Scikit-Learn findest du in der  \n",
    "üëâ [Scikit-Learn-Dokumentation zur Clusterbewertung](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "### Inertia\n",
    "\n",
    "Die **Inertia** misst die Summe der quadrierten Abst√§nde aller Datenpunkte zu den Zentroiden ihrer zugewiesenen Cluster.  \n",
    "Eine **niedrige Inertia** deutet auf kompakte, gut definierte Cluster hin.  \n",
    "Eine **hohe Inertia** spricht eher f√ºr verstreute oder √ºberlappende Gruppen.\n",
    "\n",
    "Die Inertia-Formel:\n",
    "\n",
    "$$\n",
    "\\text{Inertia} = \\sum_{j=1}^{k} \\sum_{i \\in C_j} \\|x_i - \\mu_j\\|^2\n",
    "$$\n",
    "\n",
    "Dabei ist:\n",
    "\n",
    "- \\( k \\): Anzahl der Cluster  \n",
    "- \\( C_j \\): Datenpunkte im j-ten Cluster  \n",
    "- \\( x_i \\): einzelner Datenpunkt  \n",
    "- \\( \\mu_j \\): Schwerpunkt (Zentroid) des j-ten Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Inertia f√ºr verschiedene Cluster-Anzahlen\n",
    "inertia_values = [\n",
    "    KMeans(n_clusters=k, n_init=10, random_state=42).fit(X).inertia_\n",
    "    for k in range(1, 10)\n",
    "]\n",
    "\n",
    "# In DataFrame umwandeln\n",
    "df_inertia = pd.DataFrame({\n",
    "    'Anzahl_Cluster': range(1, 10),\n",
    "    'Inertia': inertia_values\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(df_inertia['Anzahl_Cluster'], df_inertia['Inertia'], marker='o')\n",
    "plt.title(\"Elbow-Methode zur Bestimmung der Clusteranzahl\")\n",
    "plt.xlabel(\"Anzahl der Cluster (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92561977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere das KMeans-Modell mit 3 Clustern\n",
    "kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)\n",
    "\n",
    "# Trainiere das Modell auf dem vollst√§ndigen Iris-Datensatz (ohne Zielvariable)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Weist jedem Datenpunkt ein Clusterlabel (0, 1 oder 2) zu\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# Extrahiere die berechneten Zentren der Cluster (Mittelwerte der Clusterpunkte)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Funktion zur 2D-Visualisierung des Cluster-Ergebnisses anhand zweier Merkmale\n",
    "def plot_kmeans_clusters(X, y_labels, centroids, x_index=0, y_index=1, feature_names=None):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    # F√ºr jedes Cluster: Streudiagramm der zugeh√∂rigen Punkte\n",
    "    for label in range(centroids.shape[0]):\n",
    "        plt.scatter(\n",
    "            X[y_labels == label, x_index],     # Punkte dieses Clusters auf X-Achse\n",
    "            X[y_labels == label, y_index],     # Punkte dieses Clusters auf Y-Achse\n",
    "            s=50,\n",
    "            label=f'Cluster {label + 1}'\n",
    "        )\n",
    "    \n",
    "    # Plot der Cluster-Zentren (Zentroide)\n",
    "    plt.scatter(\n",
    "        centroids[:, x_index],\n",
    "        centroids[:, y_index],\n",
    "        s=300,\n",
    "        marker='*',\n",
    "        c='black',\n",
    "        label='Cluster-Zentren'\n",
    "    )\n",
    "    \n",
    "    # Achsentitel ausgeben, ggf. mit echten Merkmalsnamen\n",
    "    plt.title('KMeans-Clustering (2D-Projektion)')\n",
    "    plt.xlabel(feature_names[x_index] if feature_names else f\"Feature {x_index}\")\n",
    "    plt.ylabel(feature_names[y_index] if feature_names else f\"Feature {y_index}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Aufruf der Plotfunktion: zeige Cluster nach Sepal Length & Sepal Width\n",
    "plot_kmeans_clusters(X, y_kmeans, centers, x_index=0, y_index=1, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2719fc7-bb0d-4959-bb71-ff29250c9e55",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 24px\">Principle Component Analysis (PCA)</span>**\n",
    "\n",
    "**<span style=\"font-size: 20px\">Datensatz laden</span>**\n",
    "\n",
    "Wir laden hier zus√§tzlich die Zielvariable herunter, da wir diese zum Plotten ben√∂tigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Iris-Daten laden (Features und Zielvariable)\n",
    "iris = load_iris()\n",
    "X = iris.data           # Merkmalsmatrix\n",
    "\n",
    "color = iris.target     # Klassenlabels (f√ºr Farbzuweisung beim Plotten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d9cc-c5bc-4471-a2f8-59f0495c717a",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Erkl√§rte Varianz</span>**\n",
    "\n",
    "Die kumulierte Erkl√§rte Varianz (Cumulative Variance Ratio) in der PCA ist ein Ma√ü daf√ºr, wie viel Information von den urspr√ºnglichen Daten in den Hauptkomponenten erhalten bleibt. Es zeigt den Anteil der gesamten Varianz im Datensatz an, der von den ersten k Hauptkomponenten erkl√§rt wird. Je h√∂her der Wert, desto besser erfassen die Hauptkomponenten die Variationen im Datensatz.\n",
    "\n",
    "Wenn die kumulierte erkl√§rte Varianz 0,98 betr√§gt, bedeutet dies, dass die ersten k Hauptkomponenten zusammen 98 % der gesamten Varianz im Datensatz erkl√§ren. Mit anderen Worten, diese Hauptkomponenten erfassen einen Gro√üteil der Variationen in den Daten und bieten eine gute Zusammenfassung des Datensatzes. Dies ist oft ein Hinweis darauf, dass eine Reduktion der Dimensionen auf k Hauptkomponenten eine sinnvolle Wahl sein k√∂nnte, da sie die Daten mit relativ hoher Genauigkeit darstellen k√∂nnen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# PCA durchf√ºhren und kumulierte erkl√§rte Varianz berechnen\n",
    "pca = PCA().fit(X)\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Anzahl der Komponenten')\n",
    "plt.ylabel('Kumulierte erkl√§rte Varianz')\n",
    "plt.title('Kumulierte erkl√§rte Varianz vs. Anzahl der PCA-Komponenten')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcedd40",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Dimensionsreduktion</span>**\n",
    "\n",
    "Im n√§chsten Schritt wird die Anzahl der Merkmale mithilfe der PCA auf zwei bzw. drei Hauptkomponenten reduziert. Ziel ist es, die Daten kompakter darzustellen und gleichzeitig m√∂glichst viel Varianz beizubehalten ‚Äì ideal f√ºr Visualisierung und Clusteranalyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26410f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Iris-Daten laden (nur Features und Zielvariable f√ºr Visualisierung)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "color = iris.target\n",
    "\n",
    "# PCA mit 3 Hauptkomponenten durchf√ºhren\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# In DataFrame umwandeln f√ºr bessere √úbersicht\n",
    "X_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "# Formen vergleichen\n",
    "print(\"Urspr√ºngliche Form der Daten:\", X.shape)\n",
    "print(\"Transformierte Form (PCA):    \", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813f27-d536-4466-acd1-a0ca9f8a7fff",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Visualisierung</span>**\n",
    "\n",
    "Die Funktion `plot_combined_3d` erzeugt eine kombinierte Darstellung aus einem **3D-Scatterplot** (oben) und drei **2D-Scatterplots** (unten).\n",
    "\n",
    "- Der 3D-Plot zeigt die Datenpunkte im Raum der ersten drei Hauptkomponenten (PC1‚ÄìPC3).\n",
    "- Die 2D-Plots visualisieren jeweils die Beziehung zwischen zwei dieser Komponenten: PC1 vs. PC2, PC1 vs. PC3 und PC2 vs. PC3.\n",
    "\n",
    "Diese Darstellung hilft, Strukturen, Trennbarkeit oder potenzielle Cluster im reduzierten Merkmalsraum zu erkennen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1813d14-664e-48c8-b518-92814c84fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_combined_3d(X_pca, color):\n",
    "    # Abbildung erzeugen\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Oberer Plot: 3D-Scatterplot der ersten drei Hauptkomponenten\n",
    "    ax_3d = fig.add_subplot(2, 1, 1, projection='3d')\n",
    "    scatter_3d = ax_3d.scatter(\n",
    "        X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "        c=color, cmap='viridis', s=50, alpha=0.8\n",
    "    )\n",
    "    ax_3d.set_xlabel('PC1')\n",
    "    ax_3d.set_ylabel('PC2')\n",
    "    ax_3d.set_zlabel('PC3')\n",
    "    ax_3d.set_title('PCA ‚Äì 3D-Darstellung der ersten drei Hauptkomponenten')\n",
    "\n",
    "    # Untere Plots: alle 2D-Projektionen der drei PCs\n",
    "    for i, (x_axis, y_axis) in enumerate([(0, 1), (0, 2), (1, 2)]):\n",
    "        ax = fig.add_subplot(2, 3, i + 4)\n",
    "        ax.scatter(\n",
    "            X_pca[:, x_axis], X_pca[:, y_axis],\n",
    "            c=color, cmap='viridis', s=40, alpha=0.8\n",
    "        )\n",
    "        ax.set_xlabel(f'PC{x_axis + 1}')\n",
    "        ax.set_ylabel(f'PC{y_axis + 1}')\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Abstand optimieren\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Aufruf der Funktion (kompatibel mit DataFrame oder NumPy-Array)\n",
    "plot_combined_3d(X_pca.values if isinstance(X_pca, pd.DataFrame) else X_pca, color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cee8c-88b2-47ae-91d0-90de716c458b",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px; color: orange;\">Optional: Interaktive Visualisierung mit Plotly</span>**\n",
    "\n",
    "Die Bibliothek **Plotly** erm√∂glicht interaktive Diagramme, in denen man z.‚ÄØB. zoomen, rotieren und Datenpunkte dynamisch untersuchen kann.\n",
    "\n",
    "Um Plotly zu verwenden, muss es zun√§chst installiert werden (falls noch nicht geschehen):\n",
    "\n",
    "```bash\n",
    "pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3eaa2a-36ee-457c-9cc4-91789d9ca0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Interaktiver 3D-Scatterplot mit Plotly\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=X_pca['PC1'],\n",
    "    y=X_pca['PC2'],\n",
    "    z=X_pca['PC3'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=color,             # Farbzuordnung √ºber Klassenlabels\n",
    "        colorscale='viridis',    # Farbschema\n",
    "        size=5,                  # Punktgr√∂√üe\n",
    "        opacity=0.8              # leichte Transparenz f√ºr √úberlappung\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Layout-Anpassung: Achsentitel, gleiches Seitenverh√§ltnis\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3',\n",
    "        aspectmode='cube'  # gleichm√§√üiges Seitenverh√§ltnis\n",
    "    ),\n",
    "    title='PCA ‚Äì Interaktiver 3D-Scatterplot mit Plotly',\n",
    "    margin=dict(l=0, r=0, b=0, t=40)\n",
    ")\n",
    "\n",
    "# Plot anzeigen\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578d4e4",
   "metadata": {},
   "source": [
    "## 2.3 Kombination PCA und Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae343f4",
   "metadata": {},
   "source": [
    "## <font color='red'>Aufgabe:</font>\n",
    "\n",
    "Die Kombination aus **PCA** und **Clustering** erm√∂glicht es, Muster in hochdimensionalen Daten zu erkennen, die in der Originaldarstellung nur schwer sichtbar sind.\n",
    "\n",
    "Zun√§chst wird die Dimensionalit√§t der Daten mit **PCA** reduziert. Anschlie√üend wird ein **Clustering-Verfahren** (z.‚ÄØB. KMeans) auf den transformierten Daten (`X_pca`) angewendet.\n",
    "\n",
    "üìå **Ziel:**  \n",
    "F√ºhre das Clustering nicht auf den Originaldaten `X`, sondern auf den **dimensionsreduzierten PCA-Daten `X_pca`** durch.  \n",
    "Die resultierenden Cluster-Labels (`kmeans.predict(...)`) sollen zur **farblichen Darstellung im Scatterplot** verwendet werden.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Checkpoints:\n",
    "\n",
    "- [ ] **Daten laden** (`X`, `target`)  \n",
    "- [ ] **PCA anwenden** (`X_pca` mit 3 Hauptkomponenten)  \n",
    "- [ ] **KMeans auf `X_pca` anwenden** (Cluster-Labels berechnen)  \n",
    "- [ ] **Scatterplot erstellen** (z.‚ÄØB. mit Plotly oder Matplotlib)  \n",
    "- [ ] **Clustering-Ergebnis interpretieren** (visuell oder mit Metriken wie Silhouette Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f3ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374162e-2a75-423c-b0da-f49cf26a2c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed42c4-8f37-4f19-980b-b541e30fed27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
