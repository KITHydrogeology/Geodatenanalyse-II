{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f771b45",
   "metadata": {},
   "source": [
    "Termin 2 - Machine Learning Algorithmen (Aufgabe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7539998-4c11-4853-a763-468c4bee0142",
   "metadata": {},
   "source": [
    "# Maschinelles Lernen\n",
    "# Übung Termin 2 - Einfache Machine Learning Algorithmen\n",
    "# Kurzer Theorie-Überblick\n",
    "\n",
    "## **Überwachtes Lernen**\n",
    "\n",
    "Überwachtes Lernen bezieht sich auf eine Klasse von Algorithmen des maschinellen Lernens, bei der ein Modell mit der Verwendung von **gelabelten Trainingsdaten** trainiert wird, um eine Vorhersage zu treffen. Gelabelte Trainingsdaten sind Datensätze, die sowohl die Eingabewerte (unabhängige Variablen) als auch die zugehörigen Zielwerte (abhängige Variablen) enthalten.\n",
    "- **Klassifikation**\n",
    "    - **Binäre Klassifikation**: Eine binäre Klassifikation ist eine Form der Klassifikation im maschinellen Lernen, bei der eine Entscheidung zwischen nur **zwei Kategorien** getroffen wird. Zum Beispiel die Vorhersage, ob ein Patient an einer bestimmten Krankheit leidet oder nicht.\n",
    "\n",
    "    - **Multiklassen Klassifikation**: Multiklassenklassifikation ist eine Form der Klassifikation im maschinellen Lernen, bei der eine Entscheidung zwischen **mehr als zwei Kategorien** getroffen wird. Zum Beispiel die Klassifizierung von Bildern in verschiedene Kategorien wie Hunde, Katzen oder Vögel.\n",
    "\n",
    "- **Regression**: Regression ist ein maschinelles Lernverfahren, bei dem eine **Beziehung zwischen abhängigen und unabhängigen Variablen** modelliert wird. Zum Beispiel die Vorhersage des Grundwasserstands anhand von Daten wie der Niederschlagsmenge, der Bodenart und der Landnutzung.\n",
    "\n",
    "- **Verwendete Algorithmen**:\n",
    "    - **Support Vector Machines (SVM)**: Hohe Genauigkeit und Fähigkeit, mit großen Datensätzen umzugehen.\n",
    "    - **Random Forests (RF)**: Besonders gut geeignet für Datensätze mit vielen Features.\n",
    "\n",
    "## **Unüberwachtes Lernen**\n",
    "\n",
    "Im Gegensatz zum überwachten Lernen gibt es beim unüberwachten Lernen **keine Zielvariablen oder gelabelten Trainingsdaten**. Stattdessen zielt das unüberwachte Lernen darauf ab, **unbekannte Strukturen in den Daten zu finden** und zu extrahieren.\n",
    "\n",
    "- **Verwendete Algorithmen**:\n",
    "    - **K-Means Clustering**: Gruppiert Datenpunkte in k Klassen, um Trends zu identifizieren.\n",
    "    - **Principal Component Analysis (PCA)**: Reduziert die Dimensionalität des Feature-Space.\n",
    "    - **Kombination PCA + K-Means Clustering**: Durch die Kombination von PCA und K-Means Clustering können beide Methoden in einem Algorithmus verwendet werden, um eine bessere Vorhersagegenauigkeit zu erzielen.\n",
    " \n",
    "- <font color='purple' size='4'>Eine Liste der von scikit learn verfügbaren überwachten Lernalgorithmen findet ihr [**Hier**](https://scikit-learn.org/stable/supervised_learning.html)</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f3cc5",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Laden der Bibliotheken**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "      \n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Validation \n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Methods\n",
    "from sklearn.svm import SVC, SVR # support vector classification/regression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Adv\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc56e4",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 28px\">**Überwachtes Lernen**</span>\n",
    "\n",
    "<span style=\"font-size: 24px\">**Binäre Klassifikation**</span>\n",
    "\n",
    "Eine binäre Klassifikation ist eine Form der Klassifikation im maschinellen Lernen, bei der eine Entscheidung zwischen nur **zwei Kategorien** getroffen wird. Zum Beispiel die Vorhersage, ob ein Patient an einer bestimmten Krankheit leidet oder nicht.\n",
    "\n",
    "<span style=\"font-size: 16px\">**Datensatz einlesen und aufbereiten**</span>\n",
    "- Gegeben: Grundwasserstichtagsmesssung aus dem Jahr 2005\n",
    "- Aufgabe: Binärer Klassifikation der Landnutzung auf Basis alle Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einladen der Daten\n",
    "filename = 'gwdata_2005.csv'\n",
    "dataset = pd.read_csv(filename, delimiter=';', encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Preprocessing (Bereinigen von NaN-Werten und Entfernen von nicht benötigten Spalten)\n",
    "dataset_cleaned = dataset.dropna(axis=0, how='any')\n",
    "data = dataset_cleaned.drop(['GWNum','Messstelle','Rechtswert', 'Hochwert', 'Aquifer','Aquifer2','landuse'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a3d8d-16be-4d15-a003-100430b3b051",
   "metadata": {},
   "source": [
    "Die Spalte `landuse_num` gibt Auskunft über Landwirtschaft als binären Code.\n",
    "- 0 steht für keine Landwirtschaft (no_agr).\n",
    "- 1 steht für Landwirtschaft (agr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d08472",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl jedes eindeutigen Werts:\\n\", data['landuse_num'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67215fc",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Aufteilen der Daten in einen Test- und Trainingsdatensatz**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setzen des Seeds für die Reproduzierbarkeit\n",
    "random_state = 42\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "X = data.drop('landuse_num', axis=1)  # Spalte Zielvariable ab\n",
    "Y = data['landuse_num']  # Definiere 'landuse_num' als Zielvariable\n",
    "test_size = 0.2  # Anteil der Testdaten: 20%\n",
    "\n",
    "# Mischen und Aufteilen der Daten in Trainings- und Testdaten\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "print(\"Form der Trainingsdaten (X_train):\", X_train.shape)\n",
    "print(\"Form der Testdaten (X_test):\", X_test.shape)\n",
    "print(\"Form der Trainingszielvariablen (Y_train):\", Y_train.shape)\n",
    "print(\"Form der Testzielvariablen (Y_test):\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f9da9-d551-4799-b179-5f91c5ce1962",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Support Vector Machines (SVM)**</span>\n",
    "\n",
    "**Support Vector Machines (SVM)** sind leistungsfähige Modelle für die Klassifikation und Regression von Daten. Sie arbeiten, indem sie eine Entscheidungsgrenze zwischen den verschiedenen Klassen finden, indem sie eine Hyperebene im Feature-Raum konstruieren. \n",
    "\n",
    "[**Hier**](https://scikit-learn.org/stable/modules/svm.html) findest du weitere Informationen zur **SVM** in scikit-learn.\n",
    "\n",
    "- **Support Vector Classification (SVC)**: SVC ist eine Implementierung von SVM für die Klassifikation. Es funktioniert, indem es eine Hyperebene in einem hochdimensionalen Raum konstruiert, um die Datenpunkte so zu trennen, dass die Klassen am besten voneinander getrennt sind. SVC ist bekannt für seine Fähigkeit, auch mit komplexen Datensätzen gut umzugehen und robuste Entscheidungsgrenzen zu erzeugen.\n",
    "\n",
    "[**Hier**](https://scikit-learn.org/stable/modules/model_evaluation.html) findest du weitere Informationen zu den **Scoring-Metriken** in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zufalls-Seed für die Reproduzierbarkeit \n",
    "random_seed = 88\n",
    "# Metrik für die Kreuzvalidierung\n",
    "scoring_metric = 'accuracy'\n",
    "# Anzahl der Folds für die Kreuzvalidierung\n",
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b544c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**1/3 SVC mit unskalierten Eingangsdatedaten**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d2ed9-f247-4b4b-8587-c4ed662fcfd0",
   "metadata": {},
   "source": [
    "[Hier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) findest du weitere Informationen zur SVC in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des SVM-Modells\n",
    "svm = SVC(kernel='rbf', gamma='auto')\n",
    "\n",
    "# Definieren der K-Fold CV\n",
    "kfold = KFold(n_splits=num_folds, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung der SVM-Modells\n",
    "cv_svm = cross_val_score(svm, X_train, Y_train, cv=kfold, scoring=scoring_metric)\n",
    "\n",
    "# Berechnung der mittleren Genauigkeit und der Standardabweichung\n",
    "msg = f\"SVM Accuracy: {cv_svm.mean():.3f} ({cv_svm.std():.3f})\"\n",
    "\n",
    "# Ausgabe der SVM-Genauigkeit\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d9083",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**2/3 SVC mit skalierten Eigangsdaten**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d16910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des StandardScaler-Objekts\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Skalierung der Trainingsdaten und Erstellung des SVM-Modells\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Skalierung der Trainingsdaten\n",
    "svm_scaled = SVC(kernel='rbf', gamma='auto') # Erstellen des SVM-Modells mit RBF-Kernel und 'auto' gamma\n",
    "\n",
    "# Definieren der K-Folds CV \n",
    "kfold = KFold(n_splits=num_folds, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung der SVM-Modells\n",
    "cv_svm_scaled  = cross_val_score(svm, X_train_scaled, Y_train, cv=kfold, scoring=scoring_metric)\n",
    "\n",
    "# Berechnung der mittleren Genauigkeit und der Standardabweichung\n",
    "msg_sc = \"%f (%f)\" % (cv_svm_scaled.mean(), cv_svm_scaled.std())\n",
    "\n",
    "# Ausgabe der SVM-Genauigkeit\n",
    "print('SVM(scaled) Accuracy: ', msg_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ed19c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**<font color='blue'>For Your Interest</font>**</span>\n",
    "\n",
    "Der nächste Codeabschnitt erstellt eine Pipeline mit dem StandardScaler und dem SVM-Modell. Die Pipeline ermöglicht eine nahtlose Integration der Skalierung und des Modells in einen einzigen Prozess:\n",
    "\n",
    " Wenn nötig und die nötigen Bibliotheken installiert wurden könnt ihr viel Schritte in der Pipeline durchführen\n",
    " - Imputer (SimpleImputer(strategy='mean')): Dieser Schritt behandelt fehlende Werte im Datensatz. Es verwendet den Mittelwert der vorhandenen Daten, um fehlende Werte zu ersetzen.\n",
    " - Scaler (StandardScaler()): Dieser Schritt skaliert die Daten, um sicherzustellen, dass alle Merkmale eine ähnliche Skalenordnung haben\n",
    " - Feature Engineering (PolynomialFeatures(degree=2)): Hier werden Polynom-Features zweiten Grades zu den Daten hinzugefügt. Dies ermöglicht dem Modell, nichtlineare Beziehungen zwischen den Merkmalen zu modellieren. Die Idee ist, die Flexibilität des Modells zu erhöhen, indem man nichtlineare Entscheidungsgrenzen lernen kann.\n",
    " - Feature Selection (SelectKBest(f_classif, k=10)): Dieser Schritt wählt die besten k Features basierend auf dem ANOVA F-Test aus. Der F-Test bewertet die Signifikanz der Beziehung zwischen jedem Merkmal und der Zielvariable.\n",
    " - Dimensionality Reduction (PCA(n_components=0.95)): Hier wird eine Dimensionsreduktion mit der Hauptkomponentenanalyse (PCA) durchgeführt.\n",
    " - SVM (SVC(kernel='rbf', gamma='auto')): Dieser letzte Schritt erstellt das SVM-Modell mit einem RBF-Kernel und automatisch bestimmtem Gamma.\n",
    "\n",
    "**<span style=\"font-size: 14px\"><font color='blue'>Der Code in der nächsten Zelle dient ausschließlich zur Veranschaulichung der Funktionalität einer Pipeline für ein SVM-Modell. Beachtet, dass der Code m nicht in unserer Enviroment ausgeführt werden kann, da einige der benötigten Bibliotheken nicht installiert sind.</font></span>**   \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fab3f766-28f7-4733-9648-5ebcc660062d",
   "metadata": {},
   "source": [
    "# Erstellen der Pipeline für das SVM-Modell mit StandardScaler\n",
    "svm_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation von fehlenden Werten mit dem Mittelwert\n",
    "    ('scaler', StandardScaler()),  # Skalierung der Daten\n",
    "    ('feature_engineering', PolynomialFeatures(degree=2)),  # Hinzufügen von Polynom-Features zweiten Grades\n",
    "    ('feature_selection', SelectKBest(f_classif, k=10)),  # Auswahl der besten 10 Features basierend auf ANOVA F-Test\n",
    "    ('dimensionality_reduction', PCA(n_components=0.95)),  # Reduzierung der Dimensionalität auf 95% Varianz\n",
    "    ('svm', SVC(kernel='rbf', gamma='auto'))  # Erstellen des SVM-Modells mit RBF-Kernel und 'auto' gamma\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be77ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Pipeline für das SVM-Modell mit StandardScaler\n",
    "svm_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Skalierung der Daten\n",
    "    ('svm', SVC(kernel='rbf', gamma='auto'))  # Erstellen des SVM-Modells mit RBF-Kernel und 'auto' gamma\n",
    "])\n",
    "\n",
    "# Definieren der K-Folds CV \n",
    "kfold = KFold(n_splits=num_folds, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung des SVM-Modells\n",
    "cv_scores_svm_pipeline = cross_val_score(svm_pipeline, X_train, Y_train, cv=kfold, scoring=scoring_metric)\n",
    "\n",
    "# Berechnung der mittleren Genauigkeit und der Standardabweichung\n",
    "mean_accuracy_pipeline = cv_scores_svm_pipeline.mean()\n",
    "std_dev_accuracy_pipeline = cv_scores_svm_pipeline.std()\n",
    "\n",
    "# Ausgabe der SVM-Genauigkeit\n",
    "print(f'SVM(scaled) Accuracy: {mean_accuracy_pipeline:.3f} ({std_dev_accuracy_pipeline:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41499c27",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**3/3 Random Forest Classifier**</span>\n",
    "\n",
    "Der Random Forest Classifier ist ein Ensemble-Lernalgorithmus, der auf Entscheidungsbäumen basiert. Er kombiniert mehrere Bäume zu einem \"Wald\", wodurch Robustheit und Stabilität erhöht werden. Durch zufällige Auswahl von Features bei jedem Split und Bagging-Techniken reduziert er Überanpassung und verbessert die Generalisierungsfähigkeit. Der Random Forest ist effizient und vielseitig einsetzbar, sowohl für Klassifikation als auch Regression.\n",
    "\n",
    "[**Hier**](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier) findest du weitere Informationen zur RFC in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen des RF-Modells\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Definieren der K-Folds CV \n",
    "kfold = KFold(n_splits=10, random_state=random_seed, shuffle=True)\n",
    "\n",
    "# Kreuzvalidierung des RF-Modells\n",
    "cv_rf = cross_val_score(rf, X_train_scaled, Y_train, cv=kfold, scoring=scoring_metric)\n",
    "\n",
    "# Berechnung der mittleren Genauigkeit und der Standardabweichung\n",
    "msg_rf = \"%f (%f)\" % (cv_rf.mean(), cv_rf.std())\n",
    "\n",
    "# Ausgabe der RF-Genauigkeit\n",
    "print('RF Accuracy: ', msg_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30cc7e",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**Vergleich der Ergebnisse:**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e662b-fd56-41b1-8d72-b4a46e746fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse speichern\n",
    "results = [cv_rf, cv_svm, cv_svm_scaled]\n",
    "\n",
    "# Plotten\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(results)\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.xticks([1, 2, 3], ['RF', 'SVM', 'SVM_scaled'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e58bd1-4d7f-4823-8966-90991f16e0a9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Auswertung des besten Modells**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassifikator initialisieren\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Klassifikator auf Trainingsdaten trainieren\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "# Vorhersagen auf Testdaten treffen\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "# Genauigkeit berechnen und ausgeben\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "\n",
    "# Confusionmatrix und Klassifikationsreport berechnen und ausgeben\n",
    "conf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "class_report = classification_report(Y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef27a887-df3b-4d94-9405-aa771a4aa4ae",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 5px; background-color: black;\">\n",
    "\n",
    "\n",
    "**<span style=\"font-size: 24px\">Multiklassen Klassifikation</span>**\n",
    "\n",
    "**<span style=\"font-size: 18px\">IRIS Datensatz laden</span>**\n",
    "\n",
    "Der Iris-Datensatz enthält 150 Beispiele von Blumen, die jeweils einer von drei Arten angehören (Setosa, Versicolor oder Virginica). Er besteht aus vier numerischen Merkmalen: Länge und Breite von Sepal und Petal. Dieser Datensatz ist gut geeignet, um verschiedene Machine-Learning-Modelle auf ihre Fähigkeit zur Vorhersage der Blumenarten zu testen.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" width=\"60%\">\n",
    "\n",
    "Die Daten werden als Bunch-Objekt geladen, das neben den Eingangsmerkmalen und den Zielwerten auch Metadaten wie die Namen der Merkmale und die Namen der Klassen enthält. Ein \"bunch\" Objekt enthält in der Regel die folgenden Attribute:\n",
    "\n",
    "- **data**: Ein Numpy-Array mit den Merkmalswerten.\n",
    "- **target**: Ein Numpy-Array mit den Zielwerten.\n",
    "- **feature_names**: Eine Liste der Namen der Merkmale.\n",
    "- **target_names**: Eine Liste der Namen der Zielklassen.\n",
    "- **DESCR**: Eine Beschreibung des Datensatzes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a184fd8-b460-46e7-a061-a783fa79fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Laden des Iris-Datensatzes\n",
    "iris = load_iris()\n",
    "\n",
    "# Extrahieren der Merkmale und des Targets aus dem Iris-Datensatz\n",
    "X = iris.data  # Merkmale\n",
    "Y = iris.target  # Zielwerte\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65497b00-3242-464e-8c51-602971f1b135",
   "metadata": {},
   "source": [
    "- [x] Laden des Iris-Datensatzes\n",
    "- [ ] Teilen der Daten in Trainings- und Testdatensätze\n",
    "- [ ] Identifizieren der relevanten Eigenschaften für die Klassifikation\n",
    "- [ ] Implementieren eines SVM-Modells für die Klassifikation (unskaliert und skaliert)\n",
    "- [ ] Implementieren eines Random Forest-Modells für die Klassifikation\n",
    "- [ ] Evaluierung der Modelle mit den Testdaten und Berechnung der Genauigkeit der Vorhersagen\n",
    "- [ ] Vergleich und Diskussion der Ergebnisse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1667fee",
   "metadata": {},
   "source": [
    "\n",
    "## <font color='red'>Aufgabe:</font> \n",
    "Führe eine multiklassen Klassifikation mit dem Iris-Datensatz durch. Implementiere mindestens zwei verschiedene Modelle: Ein Modell basierend auf Support Vector Machines (SVM) für die Klassifikation (unskaliert und skaliert) und ein Modell basierend auf Random Forest (RF). Teile die Daten in Trainings- und Testdatensätze auf und identifiziere die relevanten Eigenschaften für die Klassifikation. Implementiere dann die Modelle und führe eine Evaluierung mit den Testdaten durch. Berechne die Genauigkeit der Vorhersagen für beide Modelle. Vergleiche und diskutiere abschließend die Ergebnisse und entscheide, welches Modell besser geeignet ist für die multiklassen Klassifikation des Iris-Datensatzes.\n",
    "\n",
    "<span style=\"font-size: 18px\">**Aufgabe:**</span>\n",
    "- [x] Laden des Iris-Datensatzes\n",
    "- [ ] Teilen der Daten in Trainings- und Testdatensätze\n",
    "- [ ] Identifizieren der relevanten Eigenschaften für die Klassifikation\n",
    "- [ ] Implementieren eines SVM-Modells für die Klassifikation (unskaliert und skaliert)\n",
    "- [ ] Implementieren eines Random Forest-Modells für die Klassifikation\n",
    "- [ ] Evaluierung der Modelle mit den Testdaten und Berechnung der Genauigkeit der Vorhersagen\n",
    "- [ ] Vergleich und Diskussion der Ergebnisse\n",
    "\n",
    "\n",
    "<span style=\"font-size: 18px\">**Hinweis**</span>\n",
    "\n",
    "Denke daran, die Daten angemessen vorzubereiten, einschließlich der Skalierung der Features für die SVM-Modelle, und verschiedene Hyperparameter für die Modelle zu testen, um die Leistung zu optimieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2a7ee-e5e3-4be9-bf57-dbe428eeaad8",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Preprocessing**</span>\n",
    "\n",
    "- Erstelle X_train, X_test, Y_train, Y_test.\n",
    "- Definiere tesize und den random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2755a-d68b-41bd-bdc2-39fd16bd7a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f11c1e8-b67c-4a67-8396-4ccb00bf6dfa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Support Vector Classifier**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6825f1f",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 18px\">**SVC Unskaliert**</span>\n",
    "    - Erstelle Modell\n",
    "    - Definiere K-Fold CV\n",
    "    - Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0257751-7e63-4648-8f42-fec6d32a718c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03050732-6db9-49eb-9106-eb1ed155d0c3",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 18px\">**SVC Skaliert**</span>\n",
    "    - Erstelle Modell\n",
    "    - Definiere K-Fold CV\n",
    "    - Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e836c-d9bd-4cf7-9366-47fbcc652d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e123294d-73e3-4be5-9bdc-9bf3bcf01f19",
   "metadata": {},
   "source": [
    "- <span style=\"font-size: 20px\">**Random Forest Classifier**</span>\n",
    "    - Erstelle Modell\n",
    "    - Definiere K-Fold CV\n",
    "    - Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94e810-ccc9-47c3-b44e-b69abab66bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51b2fbd-4530-4f36-9144-98993a653e7f",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px\">**Vergleich der Ergebnisse**</span>\n",
    "\n",
    "- Führe Ergebnisse zusammen und Plotte sie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb633-e063-4711-a2e0-2eec20fc1212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc21a16c-4d01-4c7d-8d14-34d647b83cd9",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 5px; background-color: black;\">\n",
    "\n",
    "**<span style=\"font-size: 24px\">Regression</span>**\n",
    "\n",
    "**<span style=\"font-size: 18px\">Concrete Datensatz</span>**\n",
    "\n",
    "Dieser Datensatz, enthält verschiedene Eigenschaften von Betonmischungen und deren Druckfestigkeit. Insgesamt enthält der Datensatz 1030 Datensätze mit 9 verschiedenen Eingabevariablen\n",
    "- Cement (Zement)\n",
    "- Blast Furnace Slag (Hüttenzement)\n",
    "- Fly Ash (Flugasche)\n",
    "- Water (Wasser)\n",
    "- Superplasticizer (Fließmittel)\n",
    "- Coarse Aggregate (Grobgestein)\n",
    "- Feingestein (Fine Aggregate)\n",
    "- Alter (Age)\n",
    "- Ausgabevariablen Druckfestigkeit (Concrete compressive strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f353cc4-f9e9-4d45-afee-558f6117c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Concrete_Data.csv', sep=';', encoding=\"ISO-8859-1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62be961-2efd-4051-a367-4910a7433432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zufalls-Seed für die Reproduzierbarkeit \n",
    "random_seed = 88\n",
    "# Metrik für die Kreuzvalidierung\n",
    "scoring_metric = 'r2'\n",
    "# Anzahl der Folds für die Kreuzvalidierung\n",
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19b685-4738-41f5-bf2a-46c9aa0e60c3",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Preprocessing</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561cebd-30f0-493c-b9a8-c4abafcfe9a2",
   "metadata": {},
   "source": [
    "Preprocing\n",
    "Führe Hier die nötigen Schritte durch um die NO3 (Y) mit den anderen Parametern vorherzusagen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84f5a4-ad33-4dbd-b32b-ec17bb147e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Löschen Übung\n",
    "# Setzen des Seeds für die Reproduzierbarkeit\n",
    "random_state = 42\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testdaten\n",
    "X = data.drop('Concrete compressive strength ', axis=1)  # Spalte Zielvariable ab\n",
    "Y = data['Concrete compressive strength ']  # Definiere 'landuse_num' als Zielvariable\n",
    "test_size = 0.2  # Anteil der Testdaten: 20%\n",
    "\n",
    "# Mischen und Aufteilen der Daten in Trainings- und Testdaten\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state, shuffle=True)\n",
    "\n",
    "print(\"Form der Trainingsdaten (X_train):\", X_train.shape)\n",
    "print(\"Form der Testdaten (X_test):\", X_test.shape)\n",
    "print(\"Form der Trainingszielvariablen (Y_train):\", Y_train.shape)\n",
    "print(\"Form der Testzielvariablen (Y_test):\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa35317-ea7b-4291-b82b-2e9a2b7f800b",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 24px\"> <font color='red'>Aufgabe: </font></span>**\n",
    "\n",
    "Entwickle zwei verschiedene Modelle zur Vorhersage der Druckfestigkeit des Betons: eines basierend auf Support Vector Machines (SVM) und eines basierend auf Random Forest (RF).\n",
    "\n",
    "<span style=\"font-size: 18px\">**Schritte:**</span>\n",
    "- [x] **Daten aufteilen:** Teile die Daten in Trainings- und Testdatensätze auf, um die Modelle zu trainieren und zu evaluiere\n",
    "- [ ] **Implementierung der Modelle:** Implementiere ein SVM-Modell und ein Random Forest-Modell.\n",
    "- [ ] **Trainiere und evaluieren:** Trainiere die Modelle mit den Trainingsdaten und evaluieren Sie sie mit den Testdaten.\n",
    "- [ ] **Berechnung der Genauigkeit:** Berechne die Genauigkeit der Vorhersagen für beide Modelle.\n",
    "- [ ] **Vergleich der Ergebnisse:** Vergleiche und diskutiere die Ergebnisse beider Modelle und entscheide, welches besser geeignet ist, um die Druckfestigkeit des Betons vorherzusagen.\n",
    "\n",
    "Alle wichtigen Infos findest du hier\n",
    "\n",
    "- [scikit-learn.org](https://scikit-learn.org)\n",
    "- [sklearn.svm.SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "- [sklearn.ensemble.RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "\n",
    "- [Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5fcbc-cc97-44ca-a8b3-364b6b58ce10",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 24px\">Support Vector Regressor</span>**\n",
    "\n",
    "**<span style=\"font-size: 18px\">Support Vector Regressor unskaliert</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700920db-401e-478f-b9ba-94201cafaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353394fc-22cf-401a-811f-e73415107448",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px\">Support Vector Regressor skaliert</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad8649-17b9-467e-9b03-da8c5bc7db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39b0de3-c720-42a1-85cd-c05c3c88f887",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px\">Random Forest skaliert</span>**\n",
    "\n",
    "- Erstelle Modell\n",
    "- Definiere K-Fold CV\n",
    "- Berechne und Speichere mittlere Genauigkeit und Standardabweichung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b7a27-15d9-448a-adf3-9d972cb13b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a89bb43-f196-4040-9053-b213f3361b5c",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px\">**Vergleich der Ergebnisse:**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc1420-18ef-4416-b1be-c1858aa77adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2fcbabd-65fc-478f-a07b-c7e604df75b7",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 5px; background-color: black;\">\n",
    "\n",
    "**<span style=\"font-size: 28px\">Unüberwachtes Lernen</span>**\n",
    "\n",
    "Die **PCA** (Principal Component Analysis) ist ein Verfahren zur Dimensionsreduktion, das verwendet wird, um die **Anzahl der Variablen in einem Datensatz zu verringern**, während gleichzeitig die **wichtigen Informationen beibehalten werden**. Im Falle des Iris-Datensatzes könnte die PCA verwendet werden, um die ursprünglichen vier Variablen (Sepal Länge, Sepal Breite, Petal Länge, Petal Breite) in eine kleinere Anzahl von Hauptkomponenten zu transformieren, die die größte Varianz im Datensatz erklären. Diese reduzierten Dimensionen können dann verwendet werden, um den Datensatz zu visualisieren, komplexe Muster zu erkennen oder die Rechenzeit für nachfolgende Analysen zu reduzieren.\n",
    "\n",
    "**KMeans** ist ein Clustering-Algorithmus, der verwendet wird, um **Datenpunkte in k Gruppen (Cluster) aufzuteilen**, wobei jedes Cluster durch **seinen Mittelpunkt (centroid) repräsentiert wird**. Im Falle des Iris-Datensatzes könnte KMeans verwendet werden, um die Datenpunkte in unterschiedliche Gruppen aufzuteilen, basierend auf den Merkmalen der Blumen. Dabei könnten beispielsweise Cluster gebildet werden, die bestimmte Arten von Iris-Blumen repräsentieren oder Cluster, die bestimmte Eigenschaften wie Petal Länge und Petal Breite gemeinsam haben. Dies könnte helfen, verborgene Strukturen im Datensatz aufzudecken und neue Erkenntnisse über die Beziehungen zwischen den Merkmalen zu gewinnen.\n",
    "\n",
    "\n",
    "**<span style=\"font-size: 24px\">KMeans Clustering</span>**\n",
    "\n",
    "**<span style=\"font-size: 20px\">Datensatz laden</span>**\n",
    "\n",
    "Wir verwenden erneut den Iris-Datensatz von vorhin. Da wir nun unüberwachtes Lernen durchführen, benötigen wir keine Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_iris()['data'] # Iris Daten -->ohne Target (Pflanzentypus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7453bea-588b-4b74-b4be-d25378e4ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der Daten ohne Clustering\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title('Iris Daten ohne Clustering')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren der KMeans-Funktion mit der Anzahl der Cluster\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "\n",
    "# Anwenden des KMeans-Algorithmus auf die Daten\n",
    "kmeans.fit(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e04de0-e760-4f4a-8fa0-ecca31aa4b11",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Validierung</span>**\n",
    "\n",
    "Für KMeans-Clustering gibt es verschiedene Metriken zur Bewertung der Clusterbildung, darunter die **Inertia**, der **Silhouette Score**, der **Calinski-Harabasz Index** und der **Davies-Bouldin Index**. Diese Metriken bieten Einblicke in die Qualität der erstellten Cluster. Weitere Informationen zu diesen Metriken und ihrer Verwendung in Scikit-Learn findest du in der [Scikit-Learn-Dokumentation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation).\n",
    "\n",
    "Die **Inertia** misst die Summe der quadrierten Abstände der Datenpunkte zu ihren jeweiligen Clusterzentren. Eine niedrige Inertia deutet darauf hin, dass die Datenpunkte in ihren Clustern eng beieinander liegen und die Cluster gut definiert sind. Eine hohe Inertia bedeutet hingegen, dass die Datenpunkte weit verstreut sind und die Cluster möglicherweise nicht gut definiert sind.\n",
    "\n",
    "**Inertia** = $\\sum_{j=1}^{k}\\sum_{i \\in C_j} ||x_i - \\mu_j||^2$\n",
    "\n",
    "wobei k die Anzahl der Cluster, Cj die Menge der Datenpunkte im j-ten Cluster, xi der i-te Datenpunkt und $\\mu_j$ der Schwerpunkt des j-ten Clusters ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung der Metriken für verschiedene Clustergrößen\n",
    "metriken = [KMeans(n_clusters=anzahl_cluster, n_init=10).fit(X).inertia_ for anzahl_cluster in range(1, 10)]\n",
    "\n",
    "# Erstellen eines Pandas Dataframes aus den Metriken und Plotten der Ergebnisse\n",
    "df_metriken = pd.DataFrame({'Anzahl_Cluster': range(1, 10), 'Metrik': metriken})\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(df_metriken['Anzahl_Cluster'], df_metriken['Metrik'], marker='o')\n",
    "plt.xlabel('Anzahl der Cluster')\n",
    "plt.ylabel('Inertia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92561977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren der KMeans-Funktion mit der Anzahl der Cluster und explizitem n_init\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "\n",
    "# Anwenden des KMeans-Algorithmus auf die Daten\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Zuordnen jedes Merkmals (Features) zu einem Cluster\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "centers= kmeans.cluster_centers_\n",
    "\n",
    "def plot_kmeans_clusters(X, y_kmeans, centroids):\n",
    "    # Erstellen des Plots\n",
    "    for cluster_label, centroid in enumerate(centroids):\n",
    "        plt.scatter(X[y_kmeans == cluster_label, 0], X[y_kmeans == cluster_label, 1], s=50, label='Cluster {}'.format(cluster_label + 1))\n",
    "    \n",
    "    # Markieren der Zentroide der Cluster\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, marker='*', label='Cluster Centroids')\n",
    "    plt.title('Iris Clustering')\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Sepal Width')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Aufruf der Funktion\n",
    "plot_kmeans_clusters(X, y_kmeans, centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2719fc7-bb0d-4959-bb71-ff29250c9e55",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 24px\">Principle Component Analysis (PCA)</span>**\n",
    "\n",
    "**<span style=\"font-size: 20px\">Datensatz laden</span>**\n",
    "\n",
    "Wir laden hier zusätzlich die Zielvariable herunter, da wir diese zum Plotten benötigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Daten\n",
    "X = load_iris()['data']\n",
    "\n",
    "# Targets dienen zum plotten\n",
    "color = load_iris()['target'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654d9cc-c5bc-4471-a2f8-59f0495c717a",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Erklärte Varianz</span>**\n",
    "\n",
    "Die kumulierte Erklärte Varianz (Cumulative Variance Ratio) in der PCA ist ein Maß dafür, wie viel Information von den ursprünglichen Daten in den Hauptkomponenten erhalten bleibt. Es zeigt den Anteil der gesamten Varianz im Datensatz an, der von den ersten k Hauptkomponenten erklärt wird. Je höher der Wert, desto besser erfassen die Hauptkomponenten die Variationen im Datensatz.\n",
    "\n",
    "Wenn die kumulierte erklärte Varianz 0,98 beträgt, bedeutet dies, dass die ersten k Hauptkomponenten zusammen 98 % der gesamten Varianz im Datensatz erklären. Mit anderen Worten, diese Hauptkomponenten erfassen einen Großteil der Variationen in den Daten und bieten eine gute Zusammenfassung des Datensatzes. Dies ist oft ein Hinweis darauf, dass eine Reduktion der Dimensionen auf k Hauptkomponenten eine sinnvolle Wahl sein könnte, da sie die Daten mit relativ hoher Genauigkeit darstellen können.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA durchführen und kumulierte erklärte Varianz berechnen\n",
    "pca = PCA().fit(X)\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')\n",
    "plt.xlabel('Anzahl der Komponenten')\n",
    "plt.ylabel('Kumulierte erklärte Varianz')\n",
    "plt.title('Kumulierte erklärte Varianz vs. Anzahl der PCA-Komponenten')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcedd40",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Dimensionsreduktion</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26410f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "X = load_iris()['data']\n",
    "color = load_iris()['target']\n",
    "\n",
    "# PCA durchführen\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "X_pca = pd.DataFrame(X_pca, columns=['PC1','PC2','PC3'])\n",
    "\n",
    "# Ausgabe der Formen der ursprünglichen und transformierten Daten\n",
    "print(\"Ursprüngliche Form der Daten:\", X.shape)\n",
    "print(\"Transformierte Form der Daten:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53813f27-d536-4466-acd1-a0ca9f8a7fff",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 20px\">Visualisierung</span>**\n",
    "\n",
    "Die Funktion plot_combined_3d erstellt einen kombinierten Plot mit einem 3D-Scatterplot oben und drei 2D-Scatterplots unten. Der 3D-Scatterplot zeigt die Datenpunkte im Raum der ersten drei Hauptkomponenten an, während die drei 2D-Scatterplots jeweils die Beziehung zwischen zwei der Hauptkomponenten darstellen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1813d14-664e-48c8-b518-92814c84fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_combined_3d(X_pca, color):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 3D Plot on top\n",
    "    ax_3d = fig.add_subplot(2, 1, 1, projection='3d')\n",
    "    ax_3d.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=color, cmap='viridis')\n",
    "    ax_3d.set_xlabel('PC1')\n",
    "    ax_3d.set_ylabel('PC2')\n",
    "    ax_3d.set_zlabel('PC3')\n",
    "    ax_3d.set_title('PCA Scatterplot in 3D')\n",
    "    \n",
    "    # 2D Scatterplots at the bottom\n",
    "    for i, (x_axis, y_axis) in enumerate([(0, 1), (0, 2), (1, 2)]):\n",
    "        ax = fig.add_subplot(2, 3, i+4)\n",
    "        ax.scatter(X_pca[:, x_axis], X_pca[:, y_axis], c=color, cmap='viridis')\n",
    "        ax.set_xlabel('PC{}'.format(x_axis+1))\n",
    "        ax.set_ylabel('PC{}'.format(y_axis+1))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_combined_3d(X_pca.values if isinstance(X_pca, pd.DataFrame) else X_pca, color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cee8c-88b2-47ae-91d0-90de716c458b",
   "metadata": {},
   "source": [
    "**<span style=\"font-size: 18px; color: orange;\">Optional: Plotly Bibliothek für interaktive Datenvisualisierung:</span>**\n",
    "\n",
    "Plotly ermöglicht die Erstellung interaktiver Diagramme, die Zoomen und Rotieren ermöglichen.\n",
    "Um Plotly zu nutzen, muss die Bibliothek zuerst in eure Umgebung installiert werden. Dies kann mit dem Befehl:\n",
    "\n",
    "``` pip install plotly ```\n",
    "\n",
    "durchgeführt werden.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3eaa2a-36ee-457c-9cc4-91789d9ca0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Erstellen des 3D-Scatterplots mit Plotly\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=X_pca['PC1'],\n",
    "    y=X_pca['PC2'],\n",
    "    z=X_pca['PC3'],\n",
    "    mode='markers',\n",
    "    marker=dict(color=color, colorscale='viridis', size=5),\n",
    ")])\n",
    "\n",
    "# Anpassen des Layouts\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title='PC1',\n",
    "        yaxis_title='PC2',\n",
    "        zaxis_title='PC3',\n",
    "        aspectmode='cube'  # 3D-Aspektverhältnis beibehalten\n",
    "    ),\n",
    "    title='PCA Scatterplot in 3D'\n",
    ")\n",
    "\n",
    "# Plot anzeigen\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578d4e4",
   "metadata": {},
   "source": [
    "## 2.3 Kombination PCA und Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae343f4",
   "metadata": {},
   "source": [
    "## <font color='red'>Aufgabe: </font>\n",
    "Die Kombination von PCA und Clustering ermöglicht es, Muster in den Daten zu erkennen, die auf den ersten Blick nicht erkennbar sind. Um dies zu erreichen, können wir zunächst eine PCA durchführen, um die Dimensionalität der Daten zu reduzieren, und dann ein Clustering auf den transformierten Daten anwenden.\n",
    "\n",
    "Um dies in der Praxis anzuwenden, können wir den Code von oben verwenden, um statt der ursprünglichen Variable X die dimensionsreduzierte Variable X_pca zu clustern. Das Ergebnis des kmeans.predict kann dann als Farbschema des Scatterplots verwendet werden, um die verschiedenen Cluster zu visualisieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1f3ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
